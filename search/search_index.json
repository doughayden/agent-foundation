{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udd16 Agent Foundation Documentation","text":"<p>Opinionated, production-ready LLM Agent deployment with enterprise-grade infrastructure.</p> <p>This template provides a complete foundation for building and deploying LLM Agents to production. Get automated CI/CD, managed state persistence, custom observability, and proven cloud infrastructure out of the box.</p> <p>Built for teams who need to move beyond prototypes and ship production AI agents with confidence.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udc33 Optimized Docker builds - Multi-stage builds with uv (~200MB images, 5-10s rebuilds)</li> <li>\ud83c\udfd7\ufe0f Automated CI/CD - GitHub Actions + Terraform with smart PR automation</li> <li>\ud83c\udf0e Multi-environment deployments - Production-grade dev/stage/prod isolation</li> <li>\ud83d\udcbe Managed sessions - Vertex AI Agent Engine for durable conversation state</li> <li>\ud83d\udd2d Custom observability - OpenTelemetry with full trace-log correlation</li> <li>\ud83d\udd10 Security - Workload Identity Federation (no service account keys)</li> </ul>"},{"location":"#documentation-guide","title":"Documentation Guide","text":""},{"location":"#first-time-setup","title":"First Time Setup","text":"<ul> <li>Getting Started - Prerequisites, bootstrap, deploy, run</li> <li>Environment Variables - Complete configuration reference</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Development - Local workflow, Docker, testing, code quality</li> <li>Infrastructure - Deployment modes, CI/CD, protection strategies, IaC</li> </ul>"},{"location":"#operations","title":"Operations","text":"<ul> <li>Observability - OpenTelemetry traces and logs</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#syncing-upstream-changes","title":"Syncing Upstream Changes","text":"<ul> <li>Template Management - Syncing upstream agent-foundation changes</li> </ul>"},{"location":"#references","title":"References","text":"<p>Deep dives for optional follow-up:</p> <ul> <li>Bootstrap - Complete bootstrap setup for both deployment modes</li> <li>Protection Strategies - Branch, tag, environment protection</li> <li>Deployment Modes - Multi-environment strategy and infrastructure</li> <li>CI/CD Workflows - Workflow architecture and mechanics</li> <li>Testing Strategy - Detailed testing patterns and organization</li> <li>Code Quality - Tool usage and exclusion strategies</li> <li>Docker Compose Workflow - Watch mode, volumes, and configuration</li> <li>Dockerfile Strategy - Multi-stage builds and optimization</li> <li>MkDocs Setup - Documentation site setup and customization</li> </ul>"},{"location":"development/","title":"Development","text":"<p>Day-to-day development workflow, code quality, testing, and Docker.</p> <p>Important</p> <p>Configure <code>AGENT_ENGINE</code> and <code>ARTIFACT_SERVICE_URI</code> in <code>.env</code> after first deployment for production-ready persistence (sessions, memory, artifacts). See Environment Variables and Getting Started.</p>"},{"location":"development/#quick-start","title":"Quick Start","text":"<pre><code># Run directly (fast iteration)\nuv run server  # API-only\nLOG_LEVEL=DEBUG uv run server  # Debug mode\nSERVE_WEB_INTERFACE=TRUE uv run server  # With web UI\n\n# Docker Compose (recommended - matches production)\ndocker compose up --build --watch\n</code></pre> <p>Prerequisites: - <code>.env</code> file configured (copy from <code>.env.example</code>) - <code>gcloud auth application-default login</code> (for Vertex AI)</p> <p>See Getting Started for initial setup.</p>"},{"location":"development/#environment-setup","title":"Environment Setup","text":"<p>Configure your local <code>.env</code> file after completing your first deployment. The deployed resources provide production-ready persistence for sessions, memory, and artifacts.</p>"},{"location":"development/#1-create-env-file","title":"1. Create .env File","text":"<pre><code>cp .env.example .env\n</code></pre>"},{"location":"development/#2-configure-environment-variables","title":"2. Configure Environment Variables","text":"<p>Edit <code>.env</code> and configure required values. The <code>.env.example</code> file includes inline comments for each variable.</p> <p>Required configuration: - Google Cloud Vertex AI credentials - Agent identification - OpenTelemetry settings</p> <p>Recommended (after first deployment): - <code>AGENT_ENGINE</code> - Session and memory persistence - <code>ARTIFACT_SERVICE_URI</code> - Artifact storage</p> <p>See Environment Variables for complete reference.</p>"},{"location":"development/#3-verify-configuration","title":"3. Verify Configuration","text":"<p>Test your setup:</p> <pre><code># Check auth\ngcloud auth application-default login\n\n# Start server\nuv run server\n\n# Or with Docker Compose\ndocker compose up --build --watch\n</code></pre> <p>Note: Without cloud resource configuration, the agent falls back to in-memory persistence (not recommended for development).</p> <p>See Environment Variables for complete reference.</p>"},{"location":"development/#feature-branch-workflow","title":"Feature Branch Workflow","text":"<pre><code># Create branch (feat/, fix/, docs/, refactor/, test/)\ngit checkout -b feat/your-feature-name\n\n# Develop locally (choose one)\ndocker compose up --build --watch  # Docker (recommended - matches production)\nuv run server  # Direct execution (if Docker unavailable)\n\n# Quality checks before commit (100% coverage required)\nuv run ruff format &amp;&amp; uv run ruff check &amp;&amp; uv run mypy\nuv run pytest --cov --cov-report=term-missing\n\n# Commit (Conventional Commits: 50 char title, list body)\ngit add . &amp;&amp; git commit -m \"feat: add new tool\"\n\n# Push and create PR\ngit push origin feat/your-feature-name\ngh pr create  # Follow PR format: What, Why, How, Tests\n\n# After merge to main, monitor deployment\ngh run list --workflow=ci-cd.yml --limit 5\ngh run view --log\n</code></pre> <p>GitHub Actions automatically builds, tests, and deploys to Cloud Run. Check job summary for deployment details.</p>"},{"location":"development/#code-quality-testing","title":"Code Quality &amp; Testing","text":"<p>Run format, lint, type check, and unit tests (100% coverage required) before every commit</p> <p>Standards: - Type Hints: Strict mypy, modern Python 3.13+ syntax (<code>|</code> unions, lowercase generics) - Code Style: Ruff enforced (88-char lines, <code>Path</code> objects, security checks) - Docstrings: Google-style format (args, returns, exceptions) - Testing: 100% coverage on production code, exclusions for configuration modules, fixtures in <code>conftest.py</code>, test behaviors and errors</p> <p>See Testing Strategy and Code Quality references for detailed patterns, tool usage, and exclusion strategies.</p>"},{"location":"development/#docker-development","title":"Docker Development","text":"<p>Recommended: Docker Compose matches production environment and enables hot reloading.</p> <p>Alternative: Use <code>uv run server</code> if Docker is unavailable (device policies, etc.). You'll need to manually restart the server when making changes.</p> <pre><code># Start with watch mode (leave running)\ndocker compose up --build --watch\n\n# Changes in src/ sync instantly\n# Changes in pyproject.toml or uv.lock trigger rebuild\n\n# Stop: Ctrl+C or docker compose down\n</code></pre> <p>Key details: - Source files sync to container without rebuild (instant feedback) - Loads <code>.env</code> automatically for configuration - Multi-stage Dockerfile optimized with uv cache mounts (~80% faster rebuilds) - Non-root container (~200MB final image)</p> <p>See Docker Compose Workflow and Dockerfile Strategy for details on watch mode, volumes, layer optimization, and security.</p>"},{"location":"development/#common-tasks","title":"Common Tasks","text":""},{"location":"development/#dependencies","title":"Dependencies","text":"<pre><code># Add runtime dependency\nuv add package-name\n\n# Add dev dependency\nuv add --group dev package-name\n\n# Update all dependencies\nuv lock --upgrade\n\n# Update specific package\nuv lock --upgrade-package package-name\n\n# After updating pyproject.toml or uv.lock:\n# - Locally: Restart server or Docker Compose (auto-rebuild with watch)\n# - CI/CD: Commit both files together (required for --locked to pass)\n</code></pre>"},{"location":"development/#version-bump","title":"Version Bump","text":"<p>When bumping version in <code>pyproject.toml</code>:</p> <pre><code># Edit version in pyproject.toml\n# Then update lockfile\nuv lock\n\n# Commit both together\ngit add pyproject.toml uv.lock\ngit commit -m \"chore: bump version to X.Y.Z\"\n</code></pre> <p>Why: CI uses <code>uv sync --locked</code> which will fail if lockfile is out of sync.</p>"},{"location":"development/#test-deployed-service","title":"Test Deployed Service","text":"<p>Proxy Cloud Run service to test locally:</p> <pre><code># Service name format: ${agent_name}-${environment}\ngcloud run services proxy &lt;service-name&gt; \\\n  --project &lt;project-id&gt; \\\n  --region &lt;region&gt; \\\n  --port 8000\n\n# Test\ncurl http://localhost:8000/health\n\n# With web UI (if SERVE_WEB_INTERFACE=TRUE)\nopen http://localhost:8000\n\n# Stop proxy: Ctrl+C\n</code></pre> <p>See Cloud Run proxy documentation.</p>"},{"location":"development/#observability","title":"Observability","text":"<p>Server Logs: - Print to stdout (via LoggingPlugin callbacks) - Basic request/response logging for immediate feedback</p> <p>Opentelemetry Traces and Logs: - Detailed traces \u2192 Cloud Trace - Structured logs \u2192 Cloud Logging - Full correlation between traces and logs</p> <p>See Observability for querying, filtering, and trace analysis.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>your-agent-name/\n  src/your_agent_name/\n    agent.py              # LlmAgent configuration\n    callbacks.py          # Agent callbacks\n    prompt.py             # Agent prompts\n    tools.py              # Custom tools\n    server.py             # FastAPI development server\n    utils/\n      config.py           # Configuration and environment parsing\n      observability.py    # OpenTelemetry setup\n  tests/                  # Test suite\n    conftest.py           # Shared fixtures\n    test_*.py             # Unit and integration tests\n  terraform/              # Infrastructure as code\n    bootstrap/{env}       # One-time CI/CD setup (per environment)\n    main/                 # Cloud Run deployment\n  docs/                   # Documentation\n  .env.example            # Environment template\n  pyproject.toml          # Project configuration\n  docker-compose.yml      # Local development\n  Dockerfile              # Container image\n  AGENTS.md               # LLM Agent instructions\n  CLAUDE.md               # Imports AGENTS.md for Claude Code\n  README.md               # Main documentation\n</code></pre> <p>\u2190 Back to Documentation</p>"},{"location":"environment-variables/","title":"Environment Variables","text":"<p>Complete configuration reference. Single source of truth.</p> <p>See <code>.env.example</code> in the repository root for template configuration with inline comments.</p>"},{"location":"environment-variables/#quick-reference","title":"Quick Reference","text":"Variable Required Default Purpose GOOGLE_GENAI_USE_VERTEXAI \u2705 - Enable Vertex AI authentication GOOGLE_CLOUD_PROJECT \u2705 - GCP project ID GOOGLE_CLOUD_LOCATION \u2705 - GCP region AGENT_NAME \u2705 - Unique agent identifier OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT \u2705 - Capture LLM content in traces AGENT_ENGINE Recommended in-memory Session/memory persistence ARTIFACT_SERVICE_URI Recommended in-memory Artifact storage LOG_LEVEL Optional <code>INFO</code> Logging verbosity TELEMETRY_NAMESPACE Optional <code>local</code> Trace grouping SERVE_WEB_INTERFACE Optional <code>FALSE</code> Enable ADK web UI RELOAD_AGENTS Optional <code>FALSE</code> Hot-reload on file changes ROOT_AGENT_MODEL Optional <code>gemini-2.5-flash</code> Override default model ALLOW_ORIGINS Optional <code>[\"http://localhost\", \"http://localhost:8000\"]</code> CORS allowed origins AGENT_DIR Optional Auto-detected Override agent directory HOST Optional <code>127.0.0.1</code> Server bind address PORT Optional <code>8000</code> Server listening port ADK_SUPPRESS_EXPERIMENTAL_FEATURE_WARNINGS Optional <code>FALSE</code> Suppress ADK warnings <p>Cloud Run auto-set: K_REVISION</p> <p>CI/CD only: TF_VAR_* variables (GitHub Actions)</p>"},{"location":"environment-variables/#required","title":"Required","text":"<p>These must be set for the agent to function.</p>"},{"location":"environment-variables/#google-cloud-vertex-ai","title":"Google Cloud Vertex AI","text":"<p>GOOGLE_GENAI_USE_VERTEXAI - Value: <code>TRUE</code> - Purpose: Enables Vertex AI authentication for Gemini models - Where: Set locally in <code>.env</code>, auto-configured in Cloud Run</p> <p>GOOGLE_CLOUD_PROJECT - Value: Your GCP project ID (e.g., <code>your-project-id</code>) - Purpose: Identifies the Google Cloud project for Vertex AI and other GCP services - Where: Set locally in <code>.env</code>, configured via Terraform for Cloud Run</p> <p>GOOGLE_CLOUD_LOCATION - Value: GCP region (e.g., <code>us-central1</code>) - Purpose: Sets the region for Vertex AI model calls and resource deployment - Where: Set locally in <code>.env</code>, configured via Terraform for Cloud Run</p>"},{"location":"environment-variables/#agent-identification","title":"Agent Identification","text":"<p>AGENT_NAME - Value: Unique identifier (e.g., <code>your-agent</code>) - Purpose: Identifies cloud resources, logs, and traces - Where: Set locally in <code>.env</code>, set before bootstrap (used for Terraform resource naming) - Note: Used as base name for Terraform resources (<code>{agent_name}-{environment}</code>)</p>"},{"location":"environment-variables/#opentelemetry","title":"OpenTelemetry","text":"<p>OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT - Options:   - <code>TRUE</code> - Capture full prompts and responses in traces   - <code>FALSE</code> - Capture metadata only (no message content) - Purpose: Controls LLM message content capture in OpenTelemetry traces - Where: Set locally in <code>.env</code>, set before bootstrap - Reference: OpenTelemetry GenAI Instrumentation - Security: Set to <code>FALSE</code> if handling sensitive data</p>"},{"location":"environment-variables/#cloud-resources","title":"Cloud Resources","text":"<p>Production-ready persistence for sessions, memory, and artifacts. Configure after first deployment.</p> <p>AGENT_ENGINE - Value: Agent Engine resource name (e.g., <code>projects/123/locations/us-central1/reasoningEngines/456</code>) - Purpose: Session and memory persistence (production-consistent behavior) - Where: Set locally in <code>.env</code> after first deployment, auto-configured in Cloud Run - How to get: GitHub Actions job summary (<code>gh run view &lt;run-id&gt;</code>) or GCP Console (Vertex AI \u2192 Agent Builder \u2192 Agent Engines) - Note: Defaults to in-memory if unset (not recommended for development)</p> <p>ARTIFACT_SERVICE_URI - Value: GCS bucket URI (e.g., <code>gs://your-artifact-bucket</code>) - Purpose: Artifact storage persistence (production-consistent behavior) - Where: Set locally in <code>.env</code> after first deployment, auto-configured in Cloud Run - How to get: GitHub Actions job summary (<code>gh run view &lt;run-id&gt;</code>) or GCP Console (Cloud Storage \u2192 Buckets) - Note: Defaults to in-memory if unset (not recommended for development)</p>"},{"location":"environment-variables/#runtime-configuration-optional","title":"Runtime Configuration (Optional)","text":""},{"location":"environment-variables/#logging","title":"Logging","text":"<p>LOG_LEVEL - Options: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> - Default: <code>INFO</code> - Purpose: Controls logging verbosity - Where: Set locally via <code>.env</code> or command line, configure via GitHub Environment Variables for Cloud Run - Usage: <pre><code>LOG_LEVEL=DEBUG uv run server\n</code></pre></p> <p>TELEMETRY_NAMESPACE - Default: <code>local</code> - Purpose: Groups traces and logs by developer or environment in Cloud Trace - Where: Set locally via <code>.env</code>, auto-set to environment name in Cloud Run deployments (dev/stage/prod) - Usage: Filter traces in Cloud Trace by namespace to isolate your development traces - Example: <code>TELEMETRY_NAMESPACE=alice-local</code></p>"},{"location":"environment-variables/#agent-features","title":"Agent Features","text":"<p>SERVE_WEB_INTERFACE - Default: <code>FALSE</code> - Purpose: Enables ADK web UI at http://127.0.0.1:8000 - Where: Set locally via <code>.env</code>, configure via GitHub Environment Variables for Cloud Run - Options:   - <code>FALSE</code> - API-only mode   - <code>TRUE</code> - Enable web interface</p> <p>RELOAD_AGENTS - Default: <code>FALSE</code> - Purpose: Enable agent hot-reloading on file changes (development only) - Where: Local development only - WARNING: Set to <code>FALSE</code> in production (Cloud Run forces <code>FALSE</code>)</p> <p>ROOT_AGENT_MODEL - Default: <code>gemini-2.5-flash</code> - Options: Any Gemini model (e.g., <code>gemini-2.5-pro</code>, <code>gemini-2.0-flash-exp</code>) - Purpose: Override default root agent model - Where: Set locally via <code>.env</code>, configure via GitHub Environment Variables for Cloud Run</p>"},{"location":"environment-variables/#cors","title":"CORS","text":"<p>ALLOW_ORIGINS - Default: <code>'[\"http://localhost\", \"http://localhost:8000\"]'</code> - Format: JSON array string - Purpose: Configure CORS allowed origins - Where: Hard-coded in Terraform for Cloud Run, configurable locally via <code>.env</code> - Example: <code>ALLOW_ORIGINS='[\"https://your-domain.com\", \"http://127.0.0.1:3000\"]'</code></p>"},{"location":"environment-variables/#advanced","title":"Advanced","text":"<p>AGENT_DIR - Default: Auto-detected (parent directory of <code>server.py</code>) - Purpose: Override agent directory path for ADK - Where: Set in Dockerfile (<code>/app/src</code>), rarely needed locally - Note: Only override if you need non-standard directory structure</p> <p>HOST - Default: <code>127.0.0.1</code> - Purpose: Server bind address - Where: Rarely needs override - defaults work for most use cases - Note: Docker Compose overrides to <code>0.0.0.0</code> in container for host access, Cloud Run manages internally</p> <p>PORT - Default: <code>8000</code> - Purpose: Server listening port - Where: Rarely needs override - Cloud Run always uses 8000, Docker Compose maps to host port 8000 - Note: Only override if you have port conflicts on your local machine</p> <p>ADK_SUPPRESS_EXPERIMENTAL_FEATURE_WARNINGS - Default: <code>FALSE</code> - Purpose: Suppress ADK experimental feature warnings - Options:   - <code>FALSE</code> - Show warnings   - <code>TRUE</code> - Suppress warnings</p>"},{"location":"environment-variables/#cloud-run-auto-set-read-only","title":"Cloud Run Auto-Set (Read-Only)","text":"<p>These variables are automatically set by Cloud Run. Do not set manually.</p> <p>K_REVISION - Purpose: Cloud Run revision identifier - Where: Auto-set by Cloud Run (used for <code>service.version</code> in traces) - Example: <code>your-agent-dev-00042-abc</code></p>"},{"location":"environment-variables/#cicd-only","title":"CI/CD Only","text":"<p>These variables are used exclusively in GitHub Actions workflows. Do not set locally.</p>"},{"location":"environment-variables/#terraform-inputs-tf_var_","title":"Terraform Inputs (TF_VAR_*)","text":"<p>GitHub Environment Variables are mapped to Terraform inputs via <code>TF_VAR_*</code> prefix:</p> <p>TF_VAR_project - Source: <code>${{ vars.GCP_PROJECT_ID }}</code> (GitHub Environment Variable) - Purpose: GCP project ID for Terraform</p> <p>TF_VAR_location - Source: <code>${{ vars.GCP_LOCATION }}</code> (GitHub Environment Variable) - Purpose: GCP region for Terraform</p> <p>TF_VAR_agent_name - Source: <code>${{ vars.IMAGE_NAME }}</code> (GitHub Environment Variable) - Purpose: Agent name for resource naming</p> <p>TF_VAR_terraform_state_bucket - Source: <code>${{ vars.TERRAFORM_STATE_BUCKET }}</code> (GitHub Environment Variable) - Purpose: GCS bucket for Terraform state</p> <p>TF_VAR_docker_image - Source: <code>${{ inputs.docker_image }}</code> (workflow input) - Purpose: Immutable image digest for deployment</p> <p>TF_VAR_environment - Source: Set by workflow (dev/stage/prod) - Purpose: Environment-specific resource naming</p>"},{"location":"environment-variables/#runtime-configuration-overrides","title":"Runtime Configuration Overrides","text":"<p>Override runtime config via GitHub Environment Variables (mapped to <code>TF_VAR_*</code>):</p> <p>TF_VAR_log_level - Source: <code>${{ vars.LOG_LEVEL }}</code> (optional GitHub Environment Variable) - Purpose: Override LOG_LEVEL for Cloud Run deployment</p> <p>TF_VAR_root_agent_model - Source: <code>${{ vars.ROOT_AGENT_MODEL }}</code> (optional GitHub Environment Variable) - Purpose: Override ROOT_AGENT_MODEL for Cloud Run deployment</p> <p>TF_VAR_serve_web_interface - Source: <code>${{ vars.SERVE_WEB_INTERFACE }}</code> (optional GitHub Environment Variable) - Purpose: Override SERVE_WEB_INTERFACE for Cloud Run deployment</p> <p>TF_VAR_otel_instrumentation_genai_capture_message_content - Source: <code>${{ vars.OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT }}</code> (optional GitHub Environment Variable) - Purpose: Override OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT for Cloud Run deployment</p>"},{"location":"environment-variables/#reference","title":"Reference","text":""},{"location":"environment-variables/#where-variables-are-set","title":"Where Variables Are Set","text":"<p>Local Development: - <code>.env</code> file (loaded via <code>python-dotenv</code>) - Command line (e.g., <code>LOG_LEVEL=DEBUG uv run server</code>) - <code>docker-compose.yml</code> (Docker Compose)</p> <p>Cloud Run: - Terraform <code>main</code> module (<code>terraform/main/main.tf</code>) - GitHub Environment Variables \u2192 <code>TF_VAR_*</code> \u2192 Terraform \u2192 Cloud Run environment</p> <p>CI/CD: - GitHub Environment Variables (auto-created by bootstrap) - Workflow inputs and outputs</p>"},{"location":"environment-variables/#security","title":"Security","text":"<ul> <li>Never commit <code>.env</code> files to version control - Already gitignored</li> <li>Workload Identity Federation - No service account keys needed for CI/CD</li> <li>Sensitive data - Set <code>OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=FALSE</code> when handling sensitive information</li> </ul> <p>\u2190 Back to Documentation</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>First-time setup: from zero to deployed.</p> <p>Note</p> <p>Bootstrap is a one-time setup per GitHub Environment. After successful bootstrap and first deployment, use the feature branch workflow described in Development.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Required: - Terraform &gt;= 1.14.0 - Google Cloud SDK (gcloud CLI) - GitHub CLI (gh) - Python 3.13+ - <code>uv</code> package manager - <code>jq</code> command-line JSON processor (for parsing Terraform output)</p> <p>GCP Project: - Create a new GCP project (or use existing) - Enable billing - Owner role</p> <p>GitHub Repository: - Create a new repository from this template - Admin access (for GitHub Environments and Variables)</p>"},{"location":"getting-started/#bootstrap-cicd","title":"Bootstrap CI/CD","text":"<p>Bootstrap creates the infrastructure for automated deployments: - Workload Identity Federation for keyless GitHub Actions authentication - Artifact Registry for Docker image storage with cleanup policies - Remote Terraform state (GCS) for bootstrap and main module \u2014 bucket created by pre-bootstrap - GitHub Environment and Variables for CI/CD</p>"},{"location":"getting-started/#1-create-state-buckets-pre-bootstrap","title":"1. Create State Buckets (Pre-Bootstrap)","text":"<p>Pre-bootstrap creates the GCS state buckets used by bootstrap and the main module. Run it before bootstrapping each environment \u2014 start with dev only, or provision all three environments at once.</p> <pre><code>cp terraform/pre/terraform.tfvars.example terraform/pre/terraform.tfvars\n# Edit terraform.tfvars: set agent_name and GCP project IDs\n\nterraform -chdir=terraform/pre init\nterraform -chdir=terraform/pre apply\n</code></pre> <p>See Bootstrap Reference: Pre-Bootstrap for scope options (dev-only, full production, incremental) and how to skip pre-bootstrap if you already have a GCS bucket.</p>"},{"location":"getting-started/#2-configure","title":"2. Configure","text":"<p>Dev-only mode (default): bootstrap only the dev environment.</p> <pre><code>cp terraform/bootstrap/dev/terraform.tfvars.example terraform/bootstrap/dev/terraform.tfvars\n# Edit terraform.tfvars with your values\n</code></pre> <p>Required variables in <code>terraform/bootstrap/dev/terraform.tfvars</code>: - <code>project</code> - GCP project ID for dev environment - <code>location</code> - GCP region (e.g., <code>us-central1</code>) - <code>agent_name</code> - Unique identifier (e.g., <code>my-agent</code>) \u2014 must match pre-bootstrap - <code>terraform_state_bucket</code> - Bucket name from pre-bootstrap output (<code>terraform_state_buckets.dev</code>) - <code>repository_owner</code> - GitHub username or organization - <code>repository_name</code> - GitHub repository name</p> <p>Refer to <code>terraform.tfvars.example</code> for required variables you customized for your agent</p> <p>Note</p> <p>For production mode (dev \u2192 stage \u2192 prod), see Infrastructure.</p>"},{"location":"getting-started/#3-authenticate","title":"3. Authenticate","text":"<pre><code>gcloud auth application-default login\ngcloud config set project YOUR_PROJECT_ID  # Optional\ngh auth login\n</code></pre>"},{"location":"getting-started/#4-bootstrap","title":"4. Bootstrap","text":"<pre><code>terraform -chdir=terraform/bootstrap/dev init \\\n  -backend-config=\"bucket=$(terraform -chdir=terraform/pre output -json terraform_state_buckets | jq -r '.dev')\"\nterraform -chdir=terraform/bootstrap/dev apply\n</code></pre>"},{"location":"getting-started/#5-verify","title":"5. Verify","text":"<pre><code># Check GitHub Variables\ngh variable list --env dev  # or GitHub repo Settings &gt; Environments &gt; dev\n</code></pre> <p>See Bootstrap Reference for complete bootstrap setup instructions.</p>"},{"location":"getting-started/#deploy","title":"Deploy","text":"<p>GitHub Actions deploy the agent resources: - Agent Engine for session and memory persistence (<code>AGENT_ENGINE</code>) - GCS bucket for artifact storage (<code>ARTIFACT_SERVICE_URI</code>) - Cloud Run service (auto-configured with all resources) - Service account with least-privilege IAM bindings - Additional cloud resources you customized for your agent</p>"},{"location":"getting-started/#1-create-pull-request","title":"1. Create Pull Request","text":"<pre><code># Create feature branch\ngit checkout -b feat/initial-setup\n\n# Commit any initial customizations\ngit add .\ngit commit -m \"feat: initial setup\"\n\n# Push branch\ngit push origin feat/initial-setup\n\n# Create PR\ngh pr create --title \"feat: initial setup\" --body \"Initial agent deployment\"\n</code></pre>"},{"location":"getting-started/#2-review-and-merge","title":"2. Review and Merge","text":"<ol> <li>Review the Terraform plan in the PR comments</li> <li>Verify the planned infrastructure changes</li> <li>Merge the PR (triggers deployment to dev environment)</li> </ol>"},{"location":"getting-started/#3-monitor-deployment","title":"3. Monitor Deployment","text":"<pre><code># View workflow runs\ngh run list --workflow=ci-cd.yml --limit 5\n\n# Watch logs (use run ID from list)\ngh run view &lt;run-id&gt; --log\n\n# Or: view in browser\ngh run view &lt;run-id&gt; --web\n</code></pre> <p>Deployment flow: 1. Build Docker image 2. Run Terraform plan (infrastructure changes) 3. Deploy to Cloud Run (default environment) 4. Report outputs in job summary</p>"},{"location":"getting-started/#4-get-deployment-info","title":"4. Get Deployment Info","text":"<pre><code># View job summary (includes Cloud Run URL, resource URIs)\ngh run view &lt;run-id&gt;\n</code></pre> <p>Save cloud resource values from the job summary to use in the next step.</p>"},{"location":"getting-started/#run-the-agent","title":"Run the Agent","text":"<p>Configure your local <code>.env</code> with cloud resource values from the deployment, then run the agent.</p>"},{"location":"getting-started/#1-create-env-file","title":"1. Create .env File","text":"<pre><code>cp .env.example .env\n</code></pre>"},{"location":"getting-started/#2-add-cloud-resource-values-for-the-local-agent","title":"2. Add Cloud Resource Values for the Local Agent","text":"<p>Add the values from the deployment job summary to <code>.env</code>:</p> <pre><code>AGENT_ENGINE=projects/YOUR_PROJECT_ID/locations/YOUR_LOCATION/reasoningEngines/YOUR_ENGINE_ID\nARTIFACT_SERVICE_URI=gs://YOUR_BUCKET_NAME\n# Add values for resources you customized for your agent\n</code></pre> <p>Enable the development web interface in your <code>.env</code>:</p> <pre><code>SERVE_WEB_INTERFACE=TRUE\n</code></pre> <p>See Environment Variables: Cloud Resources for where to find each value and a complete configuration reference.</p>"},{"location":"getting-started/#3-run-the-local-agent","title":"3. Run the Local Agent","text":"<pre><code># Authenticate with GCP (if not already done)\ngcloud auth application-default login\n\n# Run server (http://localhost:8000)\nuv run server\n\n# Or with Docker Compose (hot reloading, matches production)\ndocker compose up --build --watch\n</code></pre> <p>See Development for the full local development workflow including testing and code quality.</p>"},{"location":"getting-started/#4-run-the-remote-agent","title":"4. Run the Remote Agent","text":"<p>Authenticate to the deployed agent service using the Cloud Run proxy.</p> <pre><code># Service name format: ${AGENT_NAME}-${environment} (e.g., agent-foundation-dev)\ngcloud run services proxy &lt;service-name&gt; --project &lt;project-id&gt; --region &lt;region&gt; --port 8000\n\n# In another terminal, test the health endpoint\ncurl http://localhost:8000/health\n\n# Expected response: {\"status\": \"ok\"}\n\n# Stop proxy: Ctrl+C\n</code></pre> <p>Tip</p> <p>See Cloud Run proxy documentation for details.</p>"},{"location":"getting-started/#enable-the-deployed-remote-agent-web-interface-optional","title":"Enable the deployed remote agent web interface (optional)","text":"<p>To access the same dev UI available locally, set <code>SERVE_WEB_INTERFACE=TRUE</code> in the <code>dev</code> GitHub Environment variables and re-deploy:</p> <pre><code>gh variable set SERVE_WEB_INTERFACE --env dev --body \"TRUE\"  # Or change in GitHub on the web\n</code></pre> <p>Then re-run the latest CI/CD workflow (Actions tab \u2192 CI/CD Pipeline \u2192 Re-run jobs).</p> <p>Once deployed, the web UI is available at <code>http://localhost:8000</code> via the proxy.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Local Development: - See Development for feature branch workflow, testing, and code quality</p> <p>Infrastructure &amp; CI/CD: - See Infrastructure for deployment modes, pipeline behavior, and operations</p> <p>Observability: - View traces in Cloud Trace - View logs in Logs Explorer - See Observability for query examples</p> <p>\u2190 Back to Documentation</p>"},{"location":"infrastructure/","title":"Infrastructure","text":"<p>Deployment, CI/CD, and infrastructure management.</p> <p>Important</p> <p>Complete bootstrap setup and configure protection rules before deploying in Production Mode.</p>"},{"location":"infrastructure/#bootstrap-setup","title":"Bootstrap Setup","text":"<p>Bootstrap creates CI/CD infrastructure one time per environment: - Workload Identity Federation (keyless GitHub Actions auth) - Artifact Registry (Docker images with cleanup policies) - Terraform State Bucket (remote state for main module) - GitHub Environments and Variables - Tag protection and cross-project IAM (production mode)</p> <p>First time? See Getting Started for complete walkthrough including prerequisites.</p> <p>Setting up production mode or need full detail? See Bootstrap Reference.</p>"},{"location":"infrastructure/#protection-strategies","title":"Protection Strategies","text":"<p>Configure protection rules after bootstrap (manual setup):</p> <p>Branch Protection (main): 1. Settings \u2192 Branches \u2192 Add rule for <code>main</code> 2. Require PR with 1 approval, dismiss stale reviews 3. Require status checks: <code>Required Checks / required-status</code> 4. No force pushes, no deletions</p> <p>Tag Protection (production mode, automated): - Bootstrap creates ruleset protecting <code>v*</code> tags - Verify: Settings \u2192 Rules \u2192 Rulesets</p> <p>Environment Protection (production mode, manual): 1. Settings \u2192 Environments \u2192 prod-apply 2. Configure Required reviewers 3. Add users/teams who can approve production deployments</p> <p>See Protection Strategies for detailed UI setup and rationale.</p>"},{"location":"infrastructure/#deployment-modes","title":"Deployment Modes","text":"<p>Dev-Only Mode: - Single GCP project, single environment - Workflow: PR \u2192 plan, Merge \u2192 deploy to dev - Use: Experiments, prototypes, internal tools</p> <p>Production Mode: - Three GCP projects (dev/stage/prod) - Workflow: PR \u2192 plan, Merge \u2192 dev+stage, Tag \u2192 prod (approval required) - Use: Production services, staged deployment, compliance</p> <p>Switch modes: Edit <code>production_mode</code> in <code>.github/workflows/ci-cd.yml</code></p> <p>See Deployment Modes for mode comparison, workflow details, and switching instructions.</p>"},{"location":"infrastructure/#workflow-behavior","title":"Workflow Behavior","text":"<p>What GitHub Actions does at each trigger:</p> <p>PR Flow: - Builds Docker image \u2192 pushes to dev registry as <code>pr-{number}-{sha}</code> - Runs Terraform plan for dev - Comments plan on PR - No deployment</p> <p>Merge Flow (dev-only mode): - Builds image \u2192 pushes as <code>{sha}</code>, <code>latest</code> - Deploys to dev</p> <p>Merge Flow (production mode): - Builds image \u2192 pushes to dev registry - Deploys to dev and stage (parallel after build)</p> <p>Tag Flow (production mode): - Resolves image from stage registry - Promotes to prod registry - Deploys to prod (requires approval in prod-apply environment)</p> <p>See CI/CD Workflows for workflow architecture, job details, and customization.</p>"},{"location":"infrastructure/#deployment-operations","title":"Deployment Operations","text":""},{"location":"infrastructure/#deploy-to-dev-automatic","title":"Deploy to Dev (Automatic)","text":"<p>Create PR and merge to main: <pre><code># Create feature branch\ngit checkout -b feat/your-feature\n\n# Make changes, commit\ngit add .\ngit commit -m \"feat: add new feature\"\n\n# Push and create PR\ngit push origin feat/your-feature\ngh pr create\n\n# After approval, merge PR (via GitHub UI or CLI)\ngh pr merge --squash\n</code></pre></p> <p>GitHub Actions automatically builds and deploys to dev (+ stage in production mode) when PR is merged.</p> <p>Monitor: Actions tab \u2192 CI/CD Pipeline</p>"},{"location":"infrastructure/#enable-dev-deployment-on-pull-request-optional","title":"Enable Dev Deployment on Pull Request (Optional)","text":"<p>Note</p> <p>By default, the pipeline deploys to dev only on merge to main. Enable PR deploys to deploy dev on every PR for immediate feedback in live environment.</p> <p>Impact: - Every PR and commit deploys to dev (concurrency prevents simultaneous deploys: new commits cancel in-progress PR deploys) - Dev becomes unstable (last PR wins) - Requires team coordination for concurrent PRs - Useful for complex integrations requiring live environment testing</p> <p>Enable: Change <code>.github/workflows/ci-cd.yml</code> line 98: <pre><code># From:\nif: github.ref_type == 'branch' &amp;&amp; github.event_name == 'push'\n\n# To:\nif: github.ref_type == 'branch'\n</code></pre></p>"},{"location":"infrastructure/#deploy-to-production-production-mode","title":"Deploy to Production (Production Mode)","text":"<p>Important</p> <p>Requires manual approval gate setup: Repository admin must configure required reviewers for <code>prod-apply</code> environment (Settings \u2192 Environments \u2192 prod-apply \u2192 Required reviewers). See Protection Strategies.</p> <ol> <li>Ensure dev and stage deployments successful</li> <li>Create and push annotated tag: <pre><code>git checkout main\ngit pull\ngit tag -a v1.0.0 -m \"Release v1.0.0\"\ngit push origin v1.0.0\n</code></pre></li> <li>Monitor workflow: Actions tab \u2192 CI/CD Pipeline</li> <li>Repository admin or approved reviewer: Actions \u2192 prod-apply job \u2192 Review deployments \u2192 Approve</li> <li>Verify production deployment</li> </ol>"},{"location":"infrastructure/#rollback-production","title":"Rollback (Production)","text":"<p>Strategy 1: Cloud Run Traffic Split (instant)</p> <p>Requires GCP access: <pre><code># List revisions\ngcloud run revisions list --service=&lt;service&gt; --region=&lt;region&gt;\n\n# Rollback to previous\ngcloud run services update-traffic &lt;service&gt; \\\n  --to-revisions=&lt;previous-revision&gt;=100 \\\n  --region=&lt;region&gt;\n</code></pre></p> <p>Strategy 2: Hotfix + Tag (10-20 min)</p> <p>No GCP access needed: <pre><code># Create hotfix branch\ngit checkout -b hotfix/revert-bad-change\n\n# Revert or cherry-pick fix\ngit revert &lt;bad-commit&gt;\n\n# Push and create PR\ngit push origin hotfix/revert-bad-change\ngh pr create\n\n# After approval, merge PR\ngh pr merge --squash\n\n# Tag for production (annotated)\ngit checkout main\ngit pull\ngit tag -a v1.0.1 -m \"Hotfix: revert bad change\"\ngit push origin v1.0.1\n</code></pre></p> <p>See Troubleshooting for detailed rollback decision tree.</p>"},{"location":"infrastructure/#monitoring-deployments","title":"Monitoring Deployments","text":""},{"location":"infrastructure/#view-workflow-status","title":"View Workflow Status","text":"<p>GitHub Actions UI: 1. Actions tab \u2192 CI/CD Pipeline 2. Click run to view:    - Job summaries (deployment outputs, Cloud Run URL, resources)    - Individual job logs    - PR plan comments</p> <p>GitHub CLI: <pre><code># List recent runs\ngh run list --workflow=ci-cd.yml --limit 5\n\n# View specific run\ngh run view &lt;run-id&gt;\n\n# View logs\ngh run view &lt;run-id&gt; --log\n</code></pre></p>"},{"location":"infrastructure/#trace-deployed-image","title":"Trace Deployed Image","text":"<pre><code># Get deployed image\nIMAGE=$(gcloud run services describe &lt;service&gt; \\\n  --region &lt;region&gt; \\\n  --format='value(spec.template.spec.containers[0].image)')\n\n# Get commit SHA (first tag)\ngcloud artifacts docker images describe \"$IMAGE\" \\\n  --format=\"value(tags)\" | cut -d',' -f1\n</code></pre>"},{"location":"infrastructure/#view-traces-and-logs","title":"View Traces and Logs","text":"<p>Cloud Console: - Cloud Trace \u2192 Trace Explorer - Logs Explorer</p> <p>See Observability for query examples and trace analysis.</p>"},{"location":"infrastructure/#terraform-structure","title":"Terraform Structure","text":"<p>Bootstrap Module (<code>terraform/bootstrap/{dev,stage,prod}/</code>): - One-time CI/CD infrastructure per environment (WIF, Artifact Registry, state bucket, GitHub environments/variables) - Local state, runs manually by infrastructure owners</p> <p>Main Module (<code>terraform/main/</code>): - Application deployment: Cloud Run, service account, Agent Engine, GCS bucket - Remote state (bucket created by bootstrap), runs automatically in CI/CD</p> <p>See Deployment Modes: Terraform Structure for resource details and naming conventions.</p>"},{"location":"infrastructure/#image-promotion-production-mode","title":"Image Promotion (Production Mode)","text":"<p>Production mode promotes images between registries instead of rebuilding, ensuring exact tested bytes deploy across environments:</p> <p>Dev \u2192 Stage: Triggered on merge to main Stage \u2192 Prod: Triggered on tag push (requires manual approval)</p> <p>Cross-project IAM for promotion is configured automatically during bootstrap.</p> <p>See Deployment Modes: Image Promotion for promotion details and cross-project IAM.</p> <p>\u2190 Back to Documentation</p>"},{"location":"observability/","title":"Agent Observability with OpenTelemetry","text":"<p>This project includes production-ready OpenTelemetry observability that provides consistent behavior across local development and deployed environments. The implementation automatically instruments LLM calls and application logs with minimal configuration while coexisting with ADK's internal telemetry infrastructure.</p>"},{"location":"observability/#whats-instrumented","title":"What's Instrumented","text":"<ul> <li>LLM Operations: Google Generative AI SDK calls with request/response details</li> <li>Structured Logging: JSON logs with automatic trace correlation for Google Cloud Logging</li> <li>Agent Callbacks: Lifecycle logging for agent start/end, model calls, and tool invocations</li> </ul>"},{"location":"observability/#key-features","title":"Key Features","text":"<ul> <li>Consistent Setup: Single <code>setup_opentelemetry()</code> function used across all environments (local and deployed)</li> <li>Instance-Level Tracking: Unique <code>SERVICE_INSTANCE_ID</code> per process (PID + UUID) for collision-free identification</li> <li>Environment Grouping: <code>SERVICE_NAMESPACE</code> automatically set to environment name in deployed environments (<code>dev</code>, <code>stage</code>, <code>prod</code>)</li> <li>Version Tracking: <code>SERVICE_VERSION</code> set to Cloud Run revision ID for deployment correlation</li> <li>Google Cloud Integration: Direct export to Google Cloud Trace (OTLP) and Cloud Logging</li> <li>Trace Correlation: Logs automatically include trace context via <code>LoggingInstrumentor</code></li> <li>Service Identification: OpenTelemetry <code>service.name</code> set to <code>AGENT_NAME</code> environment variable</li> <li>Authentication: Uses Application Default Credentials (ADC) for Google Cloud APIs</li> </ul>"},{"location":"observability/#configuration","title":"Configuration","text":"<p>Required environment variables: - <code>AGENT_NAME</code>: OpenTelemetry service identifier (required) - <code>GOOGLE_CLOUD_PROJECT</code>: GCP project ID for trace and log export (required) - <code>OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT</code>: Capture LLM message content - <code>TRUE</code> or <code>FALSE</code> (required)</p> <p>Optional variables: - <code>GOOGLE_CLOUD_LOCATION</code>: Vertex AI region (default: <code>us-central1</code>) - <code>LOG_LEVEL</code>: Logging verbosity - <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> (default: <code>INFO</code>) - <code>TELEMETRY_NAMESPACE</code>: Service namespace for trace grouping (default: <code>local</code>, auto-set to workspace in deployed environments)</p> <p>See <code>.env.example</code> for complete configuration reference.</p>"},{"location":"observability/#usage","title":"Usage","text":"<p>Identical OpenTelemetry setup across local development and deployed environments: - Traces and logs automatically exported to Google Cloud - ADK web UI available locally (when web interface enabled) - Production tip: Use INFO log level to minimize logging costs</p>"},{"location":"observability/#viewing-traces-and-logs","title":"Viewing Traces and Logs","text":""},{"location":"observability/#google-cloud-console-recommended","title":"Google Cloud Console (Recommended)","text":"<p>Cloud Trace: Filter by agent name, view spans, timing, and generative AI events</p> <p>Logs Explorer: Query <code>logName=\"projects/{PROJECT_ID}/logs/{AGENT_NAME}-otel-logs\"</code> for correlated logs (replace <code>{AGENT_NAME}</code> with your agent identifier)</p>"},{"location":"observability/#gcloud-cli","title":"gcloud CLI","text":"<pre><code># Tail logs in real-time\ngcloud logging tail \"resource.type=cloud_run_revision\" --format=json\n\n# Filter by log name\ngcloud logging tail \"logName:projects/{PROJECT_ID}/logs/{AGENT_NAME}-otel-logs\"\n\n# View recent traces\ngcloud trace list --limit=10\n</code></pre>"},{"location":"observability/#vs-code-gcp-extension","title":"VS Code GCP Extension","text":"<p>Install the Google Cloud Code extension to view logs and traces directly in your IDE.</p>"},{"location":"observability/#implementation-details","title":"Implementation Details","text":"<p>Functions: <code>configure_otel_resource()</code> sets resource attributes, <code>setup_opentelemetry()</code> configures exporters</p> <p>Components: <code>GoogleGenAiSdkInstrumentor</code> (LLM ops), <code>LoggingInstrumentor</code> (trace context), <code>CloudLoggingExporter</code> (logs), <code>OTLPSpanExporter</code> (traces)</p>"},{"location":"observability/#resource-attributes","title":"Resource Attributes","text":"<p>OpenTelemetry resource attributes uniquely identify your service instances in traces and logs:</p> Attribute Source Example Description <code>service.name</code> <code>AGENT_NAME</code> env var <code>your-agent-name</code> Service identifier (set explicitly in <code>.env</code>) <code>service.namespace</code> <code>TELEMETRY_NAMESPACE</code> env var <code>dev</code>/<code>stage</code>/<code>prod</code> (deployed) or <code>local</code> (dev) Environment name grouping for traces <code>service.version</code> <code>K_REVISION</code> env var <code>your-agent-name-00042-abc</code> (deployed) or <code>local</code> (dev) Cloud Run revision or local dev indicator <code>service.instance.id</code> Generated <code>worker-1234-a1b2c3d4e5f6</code> Unique process instance (PID + UUID) <code>gcp.project_id</code> <code>GOOGLE_CLOUD_PROJECT</code> env var <code>my-project-id</code> GCP project for resource correlation <p>Local Development: - <code>service.namespace</code>: Defaults to <code>\"local\"</code> (customize via <code>TELEMETRY_NAMESPACE</code> for multi-developer disambiguation) - <code>service.version</code>: Set to <code>\"local\"</code> - <code>service.instance.id</code>: Unique per server restart (includes UUID to prevent collisions)</p> <p>Deployed Environments: - <code>service.namespace</code>: Automatically set to environment name (<code>dev</code>, <code>stage</code>, <code>prod</code>) - <code>service.version</code>: Automatically set to Cloud Run revision ID - <code>service.instance.id</code>: Unique per container instance</p>"},{"location":"observability/#callback-logging","title":"Callback Logging","text":"<p><code>LoggingCallbacks</code> (in <code>callbacks.py</code>) logs agent lifecycle events (start/end, model calls, tool invocations) with automatic trace context correlation.</p>"},{"location":"observability/#message-content-capture","title":"Message Content Capture","text":"<p><code>OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT</code> controls LLM content capture: - <code>TRUE</code>: Full content (debugging, higher costs, sensitive data) - <code>FALSE</code>: Metadata only (production, lower costs, privacy)</p> <p>Important</p> <p>Must be explicitly set to <code>TRUE</code> for ADK to capture conversation content</p>"},{"location":"observability/#resources","title":"Resources","text":"<ul> <li>Vertex AI | Agent Engine | Trace an Agent</li> <li>Google Cloud Observability | Instrument ADK Applications with OpenTelemetry</li> <li>Google Cloud Trace | View Generative AI Events</li> <li>OpenTelemetry | Generative AI Instrumentation</li> <li>OpenTelemetry | Semantic Conventions for Generative AI</li> <li>OpenTelemetry Environment Variables</li> </ul> <p>\u2190 Back to Documentation</p>"},{"location":"template-management/","title":"Template Management","text":"<p>Syncing upstream changes from the template repository.</p> <p>Tip</p> <p>First time? Read Setup and Standard Workflow. Quick Reference is your copy-paste guide.  Common Patterns and Troubleshooting are optional deep-dives.</p>"},{"location":"template-management/#philosophy","title":"Philosophy","text":"<p>This template uses transparent git-based syncing rather than opaque automation. You control what updates to pull and when, with full visibility into changes.</p> <p>Why git sync? - Transparent: Review changes before applying - Selective: Pull only what you need - Flexible: Resolve conflicts your way - No magic: Standard git commands, no proprietary tools</p>"},{"location":"template-management/#setup","title":"Setup","text":"<p>One-time configuration:</p> <pre><code># Add template repository as foundation remote\ngit remote add foundation https://github.com/your-org/agent-foundation.git\ngit remote -v  # Verify\n\n# Fetch foundation tags to refs/foundation-tags/* (avoids conflicts with local tags)\n# --no-tags prevents git from also creating local copies in refs/tags/*\n# See: https://git-scm.com/book/en/v2/Git-Internals-The-Refspec\ngit fetch foundation 'refs/tags/*:refs/foundation-tags/*' --no-tags\n</code></pre> <p>Verify foundation tags were fetched:</p> <pre><code># List foundation tags with dates\ngit for-each-ref refs/foundation-tags --format='%(refname:short) | %(creatordate:short)' --sort=-creatordate\n</code></pre> <p>Note</p> <p><code>git tag -l</code> only lists refs in <code>refs/tags/*</code>, not custom namespaces. We need <code>git for-each-ref</code> for <code>refs/foundation-tags/*</code></p>"},{"location":"template-management/#standard-workflow","title":"Standard Workflow","text":""},{"location":"template-management/#prepare","title":"Prepare","text":"<p>Check for updates <pre><code>git fetch foundation 'refs/tags/*:refs/foundation-tags/*' --no-tags\ngit for-each-ref refs/foundation-tags --format='%(refname:short)' --sort=-version:refname | head -10  # Semantic version sort (latest first)\n</code></pre></p> <p>Choose version and set variable <pre><code># Set version for copy-paste workflow (e.g., v0.9.1)\nVERSION=v0.9.1\n\n# Review what changed\ngit show foundation-tags/$VERSION:CHANGELOG.md\ngit log --oneline foundation-tags/v0.9.0..foundation-tags/$VERSION\n</code></pre></p> <p>Create sync branch <pre><code>git checkout main &amp;&amp; git pull origin main\ngit checkout -b sync/foundation-$VERSION\n</code></pre></p>"},{"location":"template-management/#sync","title":"Sync","text":"<p>Sync files in stages (see Common Patterns for detailed examples) <pre><code># Compare foundation vs current HEAD\ngit diff --stat foundation-tags/$VERSION -- . ':!src/' ':!tests/'\n\n# Stage foundation files - review with git status, edit staged files before commit\ngit checkout foundation-tags/$VERSION -- docs/\ngit commit -m \"docs: sync with $VERSION\"\n\ngit checkout foundation-tags/$VERSION -- .github/workflows/\ngit commit -m \"ci: sync workflows from $VERSION\"\n\ngit checkout foundation-tags/$VERSION -- terraform/\ngit commit -m \"infra: sync terraform from $VERSION\"\n</code></pre></p> <p>Resolve conflicts if needed <pre><code>git status\ngit mergetool\ngit add &lt;resolved-files&gt;\ngit commit --amend  # Amend most recent sync commit with resolved conflicts\n</code></pre></p> <p>Restore custom files if needed <pre><code>git checkout HEAD~1 -- docs/custom-tools.md\ngit commit --amend\n</code></pre></p>"},{"location":"template-management/#review","title":"Review","text":"<p>Add manual changes for heavily customized files <pre><code>git diff foundation-tags/$VERSION -- README.md\n# Manually edit README.md to incorporate improvements\ngit add README.md\ngit commit -m \"docs: incorporate upstream README improvements from $VERSION\"\n</code></pre></p> <p>Verify sync (customize ignore patterns for expected diffs) <pre><code>git diff --stat foundation-tags/$VERSION -- . ':!src/' ':!tests/' ':!README.md'\n</code></pre></p>"},{"location":"template-management/#test-merge","title":"Test &amp; Merge","text":"<p>Test thoroughly <pre><code>uv run ruff format &amp;&amp; uv run ruff check --fix &amp;&amp; uv run mypy\nuv run pytest --cov\ndocker compose up --build\nterraform -chdir=terraform/bootstrap/dev plan\n</code></pre></p> <p>Create PR and merge <pre><code>git push -u origin sync/foundation-$VERSION\ngh pr create --title \"Sync with foundation template $VERSION\"\n# Review and merge via GitHub\n</code></pre></p> <p>Tip</p> <p>Advanced: To sync unreleased changes from <code>foundation/main</code>, fetch the branch (<code>git fetch foundation main</code>) and replace <code>foundation-tags/$VERSION</code> with <code>foundation/main</code> in commands above.</p>"},{"location":"template-management/#quick-reference","title":"Quick Reference","text":"<p>Sync strategy by file type. Complete Setup and Prepare to create a branch and set a version (e.g., <code>VERSION=v0.9.1</code>)</p> <p>Safe to sync (review staged changes for project customizations):</p> <p>Note</p> <p><code>terraform/</code> may contain project-specific resource configurations. <code>docs/</code> may contain custom guides and environment variables. Always review staged changes before committing.</p> <pre><code>git checkout foundation-tags/$VERSION -- .github/workflows/\ngit checkout foundation-tags/$VERSION -- terraform/\ngit checkout foundation-tags/$VERSION -- docs/ mkdocs.yml\ngit checkout foundation-tags/$VERSION -- notebooks/\ngit checkout foundation-tags/$VERSION -- .gitignore .dockerignore\n# Review: git status &amp;&amp; git diff --cached\n</code></pre> <p>Review and edit manually (project-specific):</p> <pre><code># Review diffs, manually edit files to incorporate changes\ngit diff foundation-tags/$VERSION -- README.md AGENTS.md\ngit diff foundation-tags/$VERSION -- Dockerfile docker-compose.yml .env.example\ngit diff foundation-tags/$VERSION -- pyproject.toml\n</code></pre> <p>Review upstream patterns (apply manually):</p> <p>Foundation may enhance reusable code patterns. Review diffs and selectively apply improvements:</p> <pre><code># Review utils patterns (Pydantic validation, OpenTelemetry)\ngit diff foundation-tags/$VERSION -- src/agent_foundation/utils/\n\n# Review test patterns (pytest fixtures, ADK mocks)\ngit diff foundation-tags/$VERSION -- tests/conftest.py\n</code></pre> <p>Never sync (your code): - <code>src/</code> - Your agent implementation - <code>tests/</code> - Your test suite - <code>CHANGELOG.md</code> - Your version history - <code>init_template.py</code> - Removed from your project after first use - <code>LICENSE</code> - Your project license - <code>uv.lock</code> - Regenerate with <code>uv lock</code> after syncing pyproject.toml</p> <p>Warning</p> <p>After syncing <code>pyproject.toml</code>, run <code>uv lock</code> to regenerate lockfile. Never sync <code>uv.lock</code> - CI uses <code>uv sync --locked</code> which fails on stale lockfile.</p>"},{"location":"template-management/#common-patterns","title":"Common Patterns","text":"<p>Detailed examples for the Standard Workflow Sync and Review phases.</p>"},{"location":"template-management/#pull-entire-directory","title":"Pull Entire Directory","text":"<p>Warning</p> <p>Stages all files from foundation version. Overwrites local versions of tracked files in this directory. Untracked local files are not affected. Review with <code>git status</code> before committing.</p> <pre><code># Review changes (compares foundation vs current HEAD)\ngit diff foundation-tags/$VERSION -- docs/\n\n# Sync directory\ngit checkout foundation-tags/$VERSION -- docs/\ngit commit -m \"docs: sync with foundation $VERSION\"\n</code></pre>"},{"location":"template-management/#pull-specific-file","title":"Pull Specific File","text":"<pre><code># Review changes\ngit diff foundation-tags/$VERSION -- docs/deployment.md\n\n# Sync file\ngit checkout foundation-tags/$VERSION -- docs/deployment.md\ngit commit -m \"docs: sync deployment.md from $VERSION\"\n</code></pre>"},{"location":"template-management/#pull-multiple-related-files","title":"Pull Multiple Related Files","text":"<pre><code># Sync workflows\ngit checkout foundation-tags/$VERSION -- .github/workflows/\ngit commit -m \"ci: sync workflows from $VERSION\"\n\n# Sync Terraform\ngit checkout foundation-tags/$VERSION -- terraform/bootstrap/\ngit commit -m \"infra: sync bootstrap from $VERSION\"\n</code></pre>"},{"location":"template-management/#cherry-pick-specific-commits","title":"Cherry-Pick Specific Commits","text":"<pre><code># View commits between versions (adjust range as needed)\ngit log --oneline foundation-tags/v0.9.0..foundation-tags/$VERSION\n\n# Cherry-pick specific commit\ngit cherry-pick &lt;commit-sha&gt;\n\n# Or: create patch and review\ngit format-patch -1 &lt;commit-sha&gt;\ngit apply --check 0001-*.patch  # Test first\ngit apply 0001-*.patch          # Apply if clean\ngit commit -m \"feat: cherry-pick improvement from $VERSION\"\n</code></pre>"},{"location":"template-management/#resolve-conflicts","title":"Resolve Conflicts","text":"<pre><code># Attempt sync\ngit checkout foundation-tags/$VERSION -- docs/deployment.md\n\n# If conflicts occur\ngit status  # Shows conflicted files\n\n# Resolve manually (look for &lt;&lt;&lt;&lt; ==== &gt;&gt;&gt;&gt;) or use merge tool\ngit mergetool\n\n# After resolving\ngit add docs/deployment.md\ngit commit -m \"docs: merge deployment.md from $VERSION\"\n</code></pre>"},{"location":"template-management/#restore-custom-files","title":"Restore Custom Files","text":"<p>If you accidentally overwrite custom files:</p> <pre><code># Sync directory\ngit checkout foundation-tags/$VERSION -- docs/\ngit commit -m \"docs: sync with $VERSION\"\n\n# Restore custom file from previous commit\ngit checkout HEAD~1 -- docs/custom-tools.md\ngit commit --amend\n</code></pre>"},{"location":"template-management/#add-manual-changes","title":"Add Manual Changes","text":"<p>For heavily customized files (README, AGENTS.md), manually incorporate improvements:</p> <pre><code># View upstream changes\ngit diff foundation-tags/$VERSION -- README.md\ngit show foundation-tags/$VERSION:README.md  # Or view full file\n\n# Manually edit your file to incorporate useful changes, then commit\ngit add README.md\ngit commit -m \"docs: incorporate upstream README improvements from $VERSION\"\n</code></pre>"},{"location":"template-management/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Check for accidental local tag conflicts\ngit show-ref | grep -E 'refs/tags/(v[0-9])'\n\n# Delete specific local foundation tags (safe - only affects local refs)\ngit tag -d v0.9.0 v0.9.1\n</code></pre> <p>Caution</p> <p>The following commands delete ALL local tags. Verify remote state before proceeding.</p> <pre><code># Reset all local tags to origin\ngit ls-remote --tags origin  # Verify what you'll restore\ngit tag -d $(git tag -l)     # Delete all local tags\ngit fetch origin --tags      # Restore from remote\n</code></pre> <pre><code># Reset foundation-tags namespace (safe - only affects local refs)\ngit for-each-ref refs/foundation-tags --format='%(refname)' | xargs -n 1 git update-ref -d\ngit fetch foundation 'refs/tags/*:refs/foundation-tags/*' --no-tags\n</code></pre> <p>\u2190 Back to Documentation</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions.</p>"},{"location":"troubleshooting/#local-development","title":"Local Development","text":""},{"location":"troubleshooting/#docker-compose","title":"Docker Compose","text":"<p>Container keeps restarting: <pre><code># Check logs\ndocker compose logs -f\n\n# Verify .env file\ncat .env | grep GOOGLE_\n\n# Ensure gcloud auth configured\ngcloud auth application-default login\n</code></pre></p> <p>Changes not appearing: - Code changes: Should sync instantly via watch mode - Dependency changes: Watch should auto-rebuild - If stuck: Stop and restart with <code>docker compose up --build --watch</code></p> <p>Permission errors: - Data directory: Mounted read-only, should not need write access - Credentials: Ensure <code>~/.config/gcloud/application_default_credentials.json</code> exists and is readable</p> <p>Port already in use: <pre><code># Check what's using port 8000\nlsof -i :8000\n\n# Stop the conflicting process or change PORT in .env\n# (Update docker-compose.yml if changing port)\n</code></pre></p> <p>Windows path compatibility: - <code>docker-compose.yml</code> uses <code>${HOME}</code> (Unix/Mac specific) - Windows users need to update volume path:   - Replace <code>${HOME}/.config/gcloud/application_default_credentials.json</code>   - With Windows path: <code>C:\\Users\\YourUsername\\AppData\\Roaming\\gcloud\\application_default_credentials.json</code>   - Or use <code>%USERPROFILE%</code> in PowerShell</p>"},{"location":"troubleshooting/#direct-execution-uv-run-server","title":"Direct Execution (uv run server)","text":"<p>Import errors: <pre><code># Reinstall dependencies\nuv sync --locked\n\n# Check virtual environment\nuv run python -c \"import sys; print(sys.prefix)\"\n</code></pre></p> <p>Vertex AI authentication failed: <pre><code># Verify gcloud auth\ngcloud auth application-default login\n\n# Check project set correctly\ngcloud config get-value project\n\n# Verify .env variables\ncat .env | grep GOOGLE_\n</code></pre></p> <p>Module not found: <pre><code># Ensure project installed\nuv sync --locked\n\n# Verify PYTHONPATH (usually not needed with uv)\nuv run python -c \"import sys; print(sys.path)\"\n</code></pre></p>"},{"location":"troubleshooting/#cicd","title":"CI/CD","text":""},{"location":"troubleshooting/#terraform-state-lock","title":"Terraform State Lock","text":"<p>See Terraform State Lock in Terraform section below.</p>"},{"location":"troubleshooting/#workflow-not-triggering","title":"Workflow Not Triggering","text":"<p>Check path filters: - Workflows ignore documentation-only changes - See <code>ci-cd.yml</code> for complete path list - Tag triggers (<code>v*</code>) always run regardless of paths</p> <p>Verify branch protection: <pre><code># Check branch protection rules\ngh api repos/:owner/:repo/branches/main/protection\n</code></pre></p>"},{"location":"troubleshooting/#cloud-run","title":"Cloud Run","text":""},{"location":"troubleshooting/#startup-failures","title":"Startup Failures","text":"<p>Symptom: Cloud Run service won't start, health check timeout</p> <p>Common causes:</p> <p>Investigate:</p> <pre><code># 1. Check service logs for actual error\ngcloud run services logs read &lt;service-name&gt; --region &lt;region&gt; --limit 50\n\n# 2. Test locally with same config\ndocker compose up --build  # or: uv run server\n\n# 3. Verify environment variables set correctly\ngcloud run services describe &lt;service-name&gt; --region &lt;region&gt; --format=\"value(spec.template.spec.containers[0].env)\"\n</code></pre>"},{"location":"troubleshooting/#terraform","title":"Terraform","text":""},{"location":"troubleshooting/#state-lock-timeout","title":"State Lock Timeout","text":"<pre><code># Find who holds the lock\ngsutil cat gs://&lt;state-bucket&gt;/main/default.tflock\n\n# If GitHub Actions run is stuck/cancelled, force unlock\nterraform -chdir=terraform/main force-unlock &lt;lock-id&gt;\n\n# WARNING: Only force unlock if you're certain no other process is running\n</code></pre>"},{"location":"troubleshooting/#plan-drift-detected","title":"Plan Drift Detected","text":"<p>Symptom: Terraform plan shows unexpected changes</p> <p>Common causes: 1. Manual changes in GCP Console 2. Another deployment modified resources 3. Terraform state out of sync</p> <p>Solutions: <pre><code># 1. Check what changed\nterraform -chdir=terraform/main plan\n\n# 2. If manual changes were intentional, import them\nterraform -chdir=terraform/main import &lt;resource&gt; &lt;id&gt;\n\n# 3. If drift is unwanted, apply to restore desired state\nterraform -chdir=terraform/main apply\n</code></pre></p>"},{"location":"troubleshooting/#general","title":"General","text":""},{"location":"troubleshooting/#environment-variable-not-set","title":"Environment Variable Not Set","text":"<p>Symptom: Error about missing required environment variable</p> <p>Check precedence: 1. Environment variables (highest priority) 2. <code>.env</code> file (loaded via python-dotenv) 3. Default values in code (lowest priority)</p> <p>Debug: <pre><code># Check .env file\ncat .env\n\n# Check environment\nenv | grep GOOGLE_\n\n# Verify loaded in Python\nuv run python -c \"import os; from dotenv import load_dotenv; load_dotenv(); print(os.getenv('GOOGLE_CLOUD_PROJECT'))\"\n</code></pre></p>"},{"location":"troubleshooting/#version-conflicts","title":"Version Conflicts","text":"<p>Symptom: Dependency version errors or import failures</p> <p>Solutions: <pre><code># Sync dependencies from lockfile\nuv sync --locked\n\n# If lockfile stale, regenerate\nuv lock\n\n# Update specific package\nuv lock --upgrade-package &lt;package-name&gt;\n\n# Nuclear option: delete .venv and reinstall\nrm -rf .venv\nuv sync --locked\n</code></pre></p>"},{"location":"troubleshooting/#tracelog-data-not-appearing","title":"Trace/Log Data Not Appearing","text":"<p>Common causes: - <code>OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT</code> not set to <code>TRUE</code> - Wrong project ID or missing authentication - Normal delay: Traces and logs take 1-2 minutes to appear in Cloud Console</p> <p>Debug: <pre><code># Check required configuration\ncat .env | grep OTEL_\ncat .env | grep GOOGLE_\n\n# Verify auth\ngcloud auth application-default login\n\n# Run with debug logging to see OTEL errors\nLOG_LEVEL=DEBUG uv run server\n</code></pre></p>"},{"location":"troubleshooting/#rollback-strategies","title":"Rollback Strategies","text":""},{"location":"troubleshooting/#rollback-decision-tree","title":"Rollback Decision Tree","text":"<pre><code>Production issue detected?\n\u2502\n\u251c\u2500 App code regression (crashes, errors, bad behavior)\n\u2502  \u2502\n\u2502  \u251c\u2500 Have direct GCP prod access?\n\u2502  \u2502  \u2514\u2500\u2192 Strategy 1: Cloud Run Traffic Split (instant)\n\u2502  \u2502\n\u2502  \u2514\u2500 No direct GCP access?\n\u2502     \u2514\u2500\u2192 Strategy 2: Hotfix + Tag (10-20 minutes)\n\u2502\n\u251c\u2500 Bad container image (won't start, missing dependencies)\n\u2502  \u2502\n\u2502  \u251c\u2500 Old revision exists + have GCP access?\n\u2502  \u2502  \u2514\u2500\u2192 Strategy 1: Cloud Run Traffic Split (instant)\n\u2502  \u2502\n\u2502  \u2514\u2500 No old revision or no GCP access?\n\u2502     \u2514\u2500\u2192 Strategy 2: Hotfix + Tag (10-20 minutes)\n\u2502\n\u251c\u2500 Configuration regression (wrong env vars, feature flags)\n\u2502  \u2502\n\u2502  \u251c\u2500 Config in GitHub Environment variables?\n\u2502  \u2502  \u2514\u2500\u2192 Manual GitHub UI edit + re-trigger deployment\n\u2502  \u2502\n\u2502  \u2514\u2500 Config in application code?\n\u2502     \u2514\u2500\u2192 Strategy 2: Hotfix + Tag\n\u2502\n\u2514\u2500 Infrastructure regression (IAM, GCS, Cloud Run config)\n   \u2514\u2500\u2192 Strategy 2: Infrastructure Hotfix + Tag\n</code></pre>"},{"location":"troubleshooting/#strategy-1-cloud-run-traffic-split-instant","title":"Strategy 1: Cloud Run Traffic Split (Instant)","text":"<p>When to use: App code or image regression, have direct GCP access, old revision exists</p> <p>Steps: <pre><code># List revisions\ngcloud run revisions list --service=&lt;service-name&gt; --region=&lt;region&gt;\n\n# Split traffic (instant rollback to previous revision)\ngcloud run services update-traffic &lt;service-name&gt; \\\n  --to-revisions=&lt;previous-revision&gt;=100 \\\n  --region=&lt;region&gt;\n</code></pre></p> <p>Pros: - Instant rollback (seconds) - No rebuild or redeployment - Can quickly test or roll forward again</p> <p>Cons: - Requires GCP access - Only works if old revision still exists - Doesn't fix root cause (need follow-up)</p>"},{"location":"troubleshooting/#strategy-2-hotfix-tag-10-20-minutes","title":"Strategy 2: Hotfix + Tag (10-20 minutes)","text":"<p>When to use: No GCP access, or need to fix config/code permanently</p> <p>Steps: <pre><code># Create hotfix branch\ngit checkout -b hotfix/revert-bad-change\n\n# Revert or cherry-pick fix\ngit revert &lt;bad-commit&gt;\n\n# Push and create PR\ngit push origin hotfix/revert-bad-change\ngh pr create\n\n# After approval, merge PR\ngh pr merge --squash\n\n# Tag for production (annotated)\ngit checkout main\ngit pull\ngit tag -a v1.0.1 -m \"Hotfix: revert bad change\"\ngit push origin v1.0.1\n\n# Approve in prod-apply when workflow runs\n</code></pre></p> <p>Pros: - Works without GCP access - Fixes root cause (proper git history) - Goes through full CI/CD pipeline (validation)</p> <p>Cons: - Takes 10-20 minutes - Requires PR approval + prod deployment approval - Slower than traffic split</p> <p>\u2190 Back to Documentation</p>"},{"location":"references/","title":"References","text":"<p>Deep-dive technical documentation for optional follow-up.</p>"},{"location":"references/#infrastructure","title":"Infrastructure","text":"<ul> <li>Bootstrap - Complete bootstrap setup for both deployment modes</li> <li>Protection Strategies - Branch, tag, environment protection</li> <li>Deployment Modes - Multi-environment strategy and infrastructure</li> <li>CI/CD Workflows - Workflow architecture and mechanics</li> </ul>"},{"location":"references/#development","title":"Development","text":"<ul> <li>Testing Strategy - Detailed testing patterns and organization</li> <li>Code Quality - Tool usage and exclusion strategies</li> <li>Docker Compose Workflow - Watch mode, volumes, and configuration</li> <li>Dockerfile Strategy - Multi-stage builds and optimization</li> <li>MkDocs Setup - Documentation site setup and customization</li> </ul> <p>\u2190 Back to Documentation</p>"},{"location":"references/bootstrap/","title":"Bootstrap Setup Reference","text":"<p>Complete bootstrap instructions for dev-only and production modes, including cross-project IAM for image promotion.</p>"},{"location":"references/bootstrap/#overview","title":"Overview","text":"<p>Bootstrap creates one-time CI/CD infrastructure per environment:</p> <p>Resources created: 1. Workload Identity Federation - Keyless GitHub Actions authentication 2. Artifact Registry - Docker image storage with cleanup policies 3. GitHub Environments - dev/stage/prod/prod-apply (production mode) 4. GitHub Environment Variables - Auto-configured per environment 5. Tag Protection - Production tag ruleset (prod bootstrap only) 6. Cross-Project IAM - Artifact Registry reader for image promotion (stage/prod)</p> <p>State management: Remote state in GCS per environment project using <code>bootstrap/</code> prefix (bucket created by pre-bootstrap)</p> <p>Location: <code>terraform/bootstrap/{dev,stage,prod}/</code></p>"},{"location":"references/bootstrap/#pre-bootstrap","title":"Pre-Bootstrap","text":"<p>Pre-bootstrap creates GCS state buckets used by both bootstrap and the main module.</p>"},{"location":"references/bootstrap/#state-bucket-options","title":"State Bucket Options","text":"<p>Option A \u2014 Use <code>terraform/pre</code> (recommended): Creates buckets automatically and outputs their names. Enables the <code>jq</code>-based <code>terraform init</code> commands throughout this guide.</p> <p>Option B \u2014 Bring your own bucket: Skip <code>terraform/pre</code> entirely. Pass an existing GCS bucket name directly to <code>-backend-config</code> when initializing each bootstrap environment:</p> <pre><code># Replace the jq subshell with a literal bucket name\nterraform -chdir=terraform/bootstrap/dev init -backend-config=\"bucket=your-existing-bucket-name\"\n</code></pre> <p>Pre-bootstrap uses local state only (<code>terraform/pre/terraform.tfstate</code> \u2014 gitignored).</p>"},{"location":"references/bootstrap/#configure-option-a","title":"Configure (Option A)","text":"<p>Define only the environments you plan to bootstrap \u2014 you can always add more later.</p> <pre><code>cp terraform/pre/terraform.tfvars.example terraform/pre/terraform.tfvars\n</code></pre> <p><code>terraform/pre/terraform.tfvars</code>:</p> <pre><code>agent_name = \"your-agent-name\"  # Must match agent_name in bootstrap tfvars\n\nprojects = {\n  ### Always required\n  dev = \"your-project-dev\"\n\n  ### Optional, but must use stage + prod together (not one or the other)\n  # stage = \"your-project-stage\"\n  # prod  = \"your-project-prod\"\n}\n</code></pre> <p>Scope options: - Dev-only: Define only <code>dev</code> to start - Full production: Define all three (<code>dev</code>, <code>stage</code>, <code>prod</code>) up front - Incremental: Start with <code>dev</code> only; add <code>stage</code> and <code>prod</code> to <code>terraform.tfvars</code> and re-run <code>apply</code> before bootstrapping those environments \u2014 no impact on existing dev bucket</p>"},{"location":"references/bootstrap/#apply-option-a","title":"Apply (Option A)","text":"<pre><code>terraform -chdir=terraform/pre init\nterraform -chdir=terraform/pre apply\n</code></pre>"},{"location":"references/bootstrap/#note-outputs","title":"Note Outputs","text":"<pre><code>terraform -chdir=terraform/pre output\n</code></pre> <p>The <code>terraform_state_buckets</code> output provides bucket names for: - The <code>terraform_state_bucket</code> variable in each bootstrap environment's <code>terraform.tfvars</code> - The <code>-backend-config</code> flag when running <code>terraform init</code> for each bootstrap environment</p>"},{"location":"references/bootstrap/#dev-only-mode","title":"Dev-Only Mode","text":"<p>Bootstrap only the dev environment.</p>"},{"location":"references/bootstrap/#1-create-environment-config","title":"1. Create Environment Config","text":"<pre><code>cp terraform/bootstrap/dev/terraform.tfvars.example \\\n   terraform/bootstrap/dev/terraform.tfvars\n</code></pre>"},{"location":"references/bootstrap/#2-edit-configuration","title":"2. Edit Configuration","text":"<p><code>terraform/bootstrap/dev/terraform.tfvars</code>:</p> <pre><code># GCP Configuration\nproject                = \"your-dev-project-id\"\nlocation               = \"us-central1\"\nagent_name             = \"your-agent-name\"\nterraform_state_bucket = \"terraform-state-your-agent-name-dev\"  # From pre-bootstrap output\n\n# Cross-project IAM (null for dev - no promotion source)\npromotion_source_project                = null\npromotion_source_artifact_registry_name = null\n\n# GitHub Configuration\nrepository_owner = \"your-github-username-or-org\"\nrepository_name  = \"your-agent-repository\"\n\n# Optional: adjust cleanup policies (defaults shown in .example file)\n</code></pre>"},{"location":"references/bootstrap/#3-bootstrap","title":"3. Bootstrap","text":"<p>Option A - Using the bucket name created in pre-bootstrap: <pre><code>terraform -chdir=terraform/bootstrap/dev init \\\n  -backend-config=\"bucket=$(terraform -chdir=terraform/pre output -json terraform_state_buckets | jq -r '.dev')\"\nterraform -chdir=terraform/bootstrap/dev apply\n</code></pre></p> <p>Option B - Using an existing bucket (skip pre-bootstrap): <pre><code>terraform -chdir=terraform/bootstrap/dev init \\\n  -backend-config=\"bucket=your-existing-bucket-name\"\nterraform -chdir=terraform/bootstrap/dev apply\n</code></pre></p> <p>Note</p> <p>Later examples only show <code>init</code> commands using <code>jq</code> parsing from pre-bootstrap but the same existing bucket option applies</p>"},{"location":"references/bootstrap/#4-verify","title":"4. Verify","text":"<p>Check GitHub Variables: <pre><code>gh variable list --env dev\n</code></pre></p> <p>Expected variables: <code>GCP_PROJECT_ID</code>, <code>IMAGE_NAME</code>, <code>ARTIFACT_REGISTRY_URI</code>, <code>TERRAFORM_STATE_BUCKET</code>, etc.</p> <p>Check GitHub Environment: 1. Go to Settings \u2192 Environments 2. Confirm <code>dev</code> environment exists 3. Click <code>dev</code> \u2192 check Environment variables populated</p>"},{"location":"references/bootstrap/#production-mode","title":"Production Mode","text":"<p>Bootstrap all three environments sequentially (dev \u2192 stage \u2192 prod).</p> <p>Important: Stage and prod require promotion source values from previous environment bootstrap outputs.</p>"},{"location":"references/bootstrap/#1-create-config-files","title":"1. Create Config Files","text":"<pre><code># Dev\ncp terraform/bootstrap/dev/terraform.tfvars.example \\\n   terraform/bootstrap/dev/terraform.tfvars\n\n# Stage\ncp terraform/bootstrap/stage/terraform.tfvars.example \\\n   terraform/bootstrap/stage/terraform.tfvars\n\n# Prod\ncp terraform/bootstrap/prod/terraform.tfvars.example \\\n   terraform/bootstrap/prod/terraform.tfvars\n</code></pre>"},{"location":"references/bootstrap/#2-bootstrap-dev","title":"2. Bootstrap Dev","text":"<p>Edit <code>terraform/bootstrap/dev/terraform.tfvars</code>:</p> <pre><code># GCP Configuration\nproject                = \"your-dev-project-id\"\nlocation               = \"us-central1\"\nagent_name             = \"your-agent-name\"                      # MUST match pre-bootstrap agent_name\nterraform_state_bucket = \"terraform-state-your-agent-name-dev\"  # From pre-bootstrap output\n\n# Cross-project IAM (null for dev - images built in dev, no promotion source)\npromotion_source_project                = null\npromotion_source_artifact_registry_name = null\n\n# GitHub Configuration\nrepository_owner = \"your-github-username-or-org\"\nrepository_name  = \"your-agent-repository\"\n</code></pre> <p>Bootstrap:</p> <pre><code>terraform -chdir=terraform/bootstrap/dev init \\\n  -backend-config=\"bucket=$(terraform -chdir=terraform/pre output -json terraform_state_buckets | jq -r '.dev')\"\nterraform -chdir=terraform/bootstrap/dev apply\n</code></pre>"},{"location":"references/bootstrap/#3-get-dev-outputs-for-stage","title":"3. Get Dev Outputs for Stage","text":"<pre><code># Get dev project ID\nDEV_PROJECT=$(terraform -chdir=terraform/bootstrap/dev output -raw project)\n\n# Get dev registry name (format: {agent_name}-dev)\nDEV_REGISTRY=$(terraform -chdir=terraform/bootstrap/dev output -raw artifact_registry_name)\n\necho \"Dev project: $DEV_PROJECT\"\necho \"Dev registry: $DEV_REGISTRY\"\n</code></pre>"},{"location":"references/bootstrap/#4-bootstrap-stage","title":"4. Bootstrap Stage","text":"<p>Edit <code>terraform/bootstrap/stage/terraform.tfvars</code>:</p> <pre><code># GCP Configuration\nproject                = \"your-stage-project-id\"\nlocation               = \"us-central1\"\nagent_name             = \"your-agent-name\"                        # MUST match dev agent_name\nterraform_state_bucket = \"terraform-state-your-agent-name-stage\"  # From pre-bootstrap output\n\n# Cross-project IAM (use dev outputs from step 3)\npromotion_source_project                = \"your-dev-project-id\"\npromotion_source_artifact_registry_name = \"your-agent-name-dev\"\n\n# GitHub Configuration\nrepository_owner = \"your-github-username-or-org\"\nrepository_name  = \"your-agent-repository\"\n</code></pre> <p>Bootstrap:</p> <pre><code>terraform -chdir=terraform/bootstrap/stage init \\\n  -backend-config=\"bucket=$(terraform -chdir=terraform/pre output -json terraform_state_buckets | jq -r '.stage')\"\nterraform -chdir=terraform/bootstrap/stage apply\n</code></pre>"},{"location":"references/bootstrap/#5-get-stage-outputs-for-prod","title":"5. Get Stage Outputs for Prod","text":"<pre><code># Get stage project ID\nSTAGE_PROJECT=$(terraform -chdir=terraform/bootstrap/stage output -raw project)\n\n# Get stage registry name (format: {agent_name}-stage)\nSTAGE_REGISTRY=$(terraform -chdir=terraform/bootstrap/stage output -raw artifact_registry_name)\n\necho \"Stage project: $STAGE_PROJECT\"\necho \"Stage registry: $STAGE_REGISTRY\"\n</code></pre>"},{"location":"references/bootstrap/#6-bootstrap-prod","title":"6. Bootstrap Prod","text":"<p>Edit <code>terraform/bootstrap/prod/terraform.tfvars</code>:</p> <pre><code># GCP Configuration\nproject                = \"your-prod-project-id\"\nlocation               = \"us-central1\"\nagent_name             = \"your-agent-name\"                       # MUST match dev/stage agent_name\nterraform_state_bucket = \"terraform-state-your-agent-name-prod\"  # From pre-bootstrap output\n\n# Cross-project IAM (use stage outputs from step 5)\npromotion_source_project                = \"your-stage-project-id\"\npromotion_source_artifact_registry_name = \"your-agent-name-stage\"\n\n# GitHub Configuration\nrepository_owner = \"your-github-username-or-org\"\nrepository_name  = \"your-agent-repository\"\n</code></pre> <p>Bootstrap:</p> <pre><code>terraform -chdir=terraform/bootstrap/prod init \\\n  -backend-config=\"bucket=$(terraform -chdir=terraform/pre output -json terraform_state_buckets | jq -r '.prod')\"\nterraform -chdir=terraform/bootstrap/prod apply\n</code></pre>"},{"location":"references/bootstrap/#7-verify-all-environments","title":"7. Verify All Environments","text":"<p>Check GitHub Environments: 1. Settings \u2192 Environments 2. Confirm environments: <code>dev</code>, <code>stage</code>, <code>prod</code>, <code>prod-apply</code></p> <p>Check Tag Protection: 1. Settings \u2192 Rules \u2192 Rulesets 2. Confirm ruleset: Production Release Tag Protection 3. Check: Enforcement=Active, Target=Tags, Patterns=<code>refs/tags/v*</code></p> <p>Check Environment Variables: 1. Settings \u2192 Environments \u2192 click each (<code>dev</code>, <code>stage</code>, <code>prod</code>) 2. Environment variables tab \u2192 verify values populated</p> <p>Verify with CLI: <pre><code># Check environments\ngh api repos/:owner/:repo/environments | jq -r '.environments[].name'\n\n# Check tag protection ruleset\ngh api repos/:owner/:repo/rulesets | jq '.[] | {name, enforcement, target}'\n</code></pre></p>"},{"location":"references/bootstrap/#8-configure-prod-apply-reviewers-required","title":"8. Configure prod-apply Reviewers (REQUIRED)","text":"<ol> <li>Settings \u2192 Environments \u2192 prod-apply</li> <li>Under Environment protection rules, check Required reviewers</li> <li>Click Add reviewers</li> <li>Search for and add users or teams who can approve production deployments</li> <li>Click Save protection rules</li> </ol> <p>See Protection Strategies for detailed setup instructions.</p>"},{"location":"references/bootstrap/#important-notes","title":"Important Notes","text":"<p>Migrating Existing Local Bootstrap State: - If you bootstrapped before this change, your existing state is local (<code>terraform/bootstrap/{env}/terraform.tfstate</code>) - Pass <code>-migrate-state</code> to copy it to GCS during init: <pre><code>terraform -chdir=terraform/bootstrap/dev init \\\n  -backend-config=\"bucket=$(terraform -chdir=terraform/pre output -json terraform_state_buckets | jq -r '.dev')\" \\\n  -migrate-state\n</code></pre> - Delete the local state file after successful migration</p> <p>Sequential Bootstrap: - Production mode requires bootstrapping in order: dev \u2192 stage \u2192 prod - Stage needs dev outputs (promotion_source_project, promotion_source_artifact_registry_name) - Prod needs stage outputs</p> <p>Agent Name Consistency: - <code>agent_name</code> MUST be identical across pre-bootstrap and all bootstrap environments - Used in resource naming: <code>{agent_name}-{environment}</code> - Example: <code>my-agent-dev</code>, <code>my-agent-stage</code>, <code>my-agent-prod</code></p> <p>Different GCP Projects: - Use separate GCP projects for each environment (security and cost isolation) - Example: <code>my-company-dev</code>, <code>my-company-stage</code>, <code>my-company-prod</code></p> <p>Cross-Project IAM: - Grants read-only access for image promotion - Registry-scoped (not project-level) - Stage WIF principal \u2192 read dev registry - Prod WIF principal \u2192 read stage registry</p> <p>GitHub Environments (Production Mode): - <code>dev</code>, <code>stage</code>, <code>prod</code> - Standard deployment environments - <code>prod-apply</code> - Separate environment for approval gate (manual reviewers)</p>"},{"location":"references/bootstrap/#bootstrap-outputs","title":"Bootstrap Outputs","text":"<p>Key outputs (use for downstream configuration):</p> <pre><code># View all outputs\nterraform -chdir=terraform/bootstrap/{env} output\n\n# Specific outputs\nterraform -chdir=terraform/bootstrap/dev output -raw project\nterraform -chdir=terraform/bootstrap/dev output -raw artifact_registry_name\nterraform -chdir=terraform/bootstrap/dev output -raw terraform_state_bucket\n</code></pre> <p>Note: <code>terraform_state_bucket</code> is an input to bootstrap (sourced from pre-bootstrap outputs via <code>terraform.tfvars</code>) that bootstrap passes through to GitHub Environment Variables. Bootstrap does not create this bucket \u2014 pre-bootstrap does.</p> <p>Use cases: - Promotion variables for next environment bootstrap - Troubleshooting WIF authentication - Verifying resource names</p> <p>\u2190 Back to References | Documentation</p>"},{"location":"references/cicd/","title":"CI/CD Workflows Reference","text":"<p>GitHub Actions workflow architecture, mechanics, and customization.</p>"},{"location":"references/cicd/#workflow-architecture","title":"Workflow Architecture","text":"<p>Orchestrator: - <code>ci-cd.yml</code> - Main workflow coordinating all jobs based on trigger event</p> <p>Reusable Workflows: - <code>config-summary.yml</code> - Configuration and production mode detection - <code>metadata-extract.yml</code> - Build metadata extraction - <code>docker-build.yml</code> - Docker image build and push - <code>pull-and-promote.yml</code> - Image promotion between registries (production mode) - <code>resolve-image-digest.yml</code> - Digest lookup by tag (production mode) - <code>terraform-plan-apply.yml</code> - Terraform deployment - <code>code-quality.yml</code> - Code quality checks (ruff, mypy, pytest) - <code>required-checks.yml</code> - Conditional status check wrapper</p> <p>Key principle: Infrastructure as code + GitOps = reproducible deployments.</p>"},{"location":"references/cicd/#github-variables-auto-created-by-bootstrap","title":"GitHub Variables (Auto-Created by Bootstrap)","text":"<p>Dev-only mode: - Variables scoped to repository (no environments)</p> <p>Production mode: - Variables scoped to environments (dev/stage/prod)</p> Variable Name Description <code>GCP_PROJECT_ID</code> GCP project ID <code>GCP_LOCATION</code> GCP region <code>IMAGE_NAME</code> Docker image name (also agent_name) <code>GCP_WORKLOAD_IDENTITY_PROVIDER</code> WIF provider resource name <code>ARTIFACT_REGISTRY_URI</code> Registry URI <code>ARTIFACT_REGISTRY_LOCATION</code> Registry location <code>TERRAFORM_STATE_BUCKET</code> GCS bucket for main module state <code>OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT</code> Capture LLM content in traces <p>Note: These are Variables (not Secrets) because they're resource identifiers, not credentials. Security comes from WIF IAM policies.</p>"},{"location":"references/cicd/#ci-cdyml-orchestrator","title":"ci-cd.yml (Orchestrator)","text":"<p>Triggers: - Pull request to main (paths filtered) - Push to main (paths filtered) - Tag push matching <code>v*</code></p> <p>Key jobs: - <code>meta</code> - Extract metadata (tags, SHA, context) - <code>config</code> - Determine production mode - <code>build</code> - Build Docker image (branch events only, not tags) - <code>resolve-digest</code> - Look up image in stage by tag (tag events in production mode) - <code>dev-plan</code> / <code>dev-apply</code> - Dev environment (branch events) - <code>stage-promote</code> / <code>stage-plan</code> / <code>stage-apply</code> - Stage environment (merge in production mode) - <code>prod-promote</code> / <code>prod-plan</code> / <code>prod-apply</code> - Prod environment (tags in production mode)</p> <p>Concurrency: - PR builds: Cancel in-progress on new push (<code>cancel-in-progress: true</code>) - Main builds: Run sequentially (no cancellation, <code>cancel-in-progress: false</code>) - Per-environment Terraform locking prevents state corruption</p> <p>Path filtering: <pre><code>paths:\n  - 'src/**'\n  - 'pyproject.toml'\n  - 'uv.lock'\n  - 'Dockerfile'\n  - '.dockerignore'\n  - 'terraform/main/**'\n  - '.github/workflows/ci-cd.yml'\n  - '.github/workflows/config-summary.yml'\n  - '.github/workflows/docker-build.yml'\n  - '.github/workflows/metadata-extract.yml'\n  - '.github/workflows/pull-and-promote.yml'\n  - '.github/workflows/resolve-image-digest.yml'\n  - '.github/workflows/terraform-plan-apply.yml'\n</code></pre></p> <p>Tag triggers (<code>v*</code>) always run regardless of paths.</p>"},{"location":"references/cicd/#workflow-flows","title":"Workflow Flows","text":"<p>Job-level dependency graphs showing how GitHub Actions jobs chain together. For the higher-level deployment strategy view, see Deployment Modes: Deployment Flow.</p>"},{"location":"references/cicd/#pr-flow","title":"PR Flow","text":"<p>Trigger: Push to feature branch with open PR</p> <p>What happens (both modes): <pre><code>config \u2192 metadata-extract \u2192 docker-build \u2192 dev-plan\n                              \u2193\n                           Push to dev registry: pr-{number}-{sha}\n                              \u2193\n                           Terraform plan (no apply)\n                              \u2193\n                           Comment plan on PR\n</code></pre></p> <p>Result: Plan preview in PR comment, no actual deployment.</p>"},{"location":"references/cicd/#merge-flow","title":"Merge Flow","text":"<p>Dev-only mode: <pre><code>config \u2192 metadata-extract \u2192 docker-build \u2192 dev-plan \u2192 dev-apply\n                              \u2193\n                           Push to dev registry: {sha}, latest\n                              \u2193\n                           Deploy to dev Cloud Run\n</code></pre></p> <p>Production mode: <pre><code>config \u2192 metadata-extract \u2192 docker-build\n           \u2193                    \u2193\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2192 dev-plan \u2192 dev-apply\n                                    (parallel)\n                                \u2193\n                              stage-promote \u2192 stage-plan \u2192 stage-apply\n                                \u2193\n                           Pull from dev, push to stage\n                                \u2193\n                           Deploy to stage Cloud Run\n</code></pre></p> <p>Result: Dev deployed (always), stage deployed (production mode only).</p>"},{"location":"references/cicd/#tag-flow","title":"Tag Flow","text":"<p>Dev-only mode: <pre><code>config \u2192 metadata-extract \u2192 docker-build \u2192 dev-plan \u2192 dev-apply\n                              \u2193\n                           Push to dev registry: {sha}, latest, {version}\n                              \u2193\n                           Deploy to dev Cloud Run\n</code></pre></p> <p>Production mode: <pre><code>config \u2192 metadata-extract \u2192 resolve-digest \u2192 prod-promote \u2192 prod-plan \u2192 prod-apply\n                              \u2193                  \u2193                          \u2191\n                           Look up image in     Pull from stage         (requires\n                           stage by tag         Push to prod             approval)\n                              \u2193\n                           Deploy to prod Cloud Run (after approval)\n</code></pre></p> <p>Result: Version-tagged deployment. Prod requires manual approval in <code>prod-apply</code> environment.</p>"},{"location":"references/cicd/#image-tagging-strategy","title":"Image Tagging Strategy","text":"<p>Pull Request builds: - Format: <code>pr-{number}-{sha}</code> (e.g., <code>pr-123-abc1234</code>) - Isolated from main builds - Tagged for dev registry only</p> <p>Main branch builds: - Tags: <code>{sha}</code> (primary), <code>latest</code> - Example: <code>abc1234</code>, <code>latest</code></p> <p>Version tag builds: - Tags: <code>{sha}</code>, <code>latest</code>, <code>{version}</code> - Example: <code>abc1234</code>, <code>latest</code>, <code>v1.0.0</code></p> <p>Deployment uses image digest (not tags) to ensure every rebuild triggers a new Cloud Run revision.</p>"},{"location":"references/cicd/#reusable-workflows","title":"Reusable Workflows","text":""},{"location":"references/cicd/#config-summaryyml","title":"config-summary.yml","text":"<p>Purpose: Determine deployment mode and create configuration summary.</p> <p>Inputs: - <code>production_mode</code> (boolean) - Enable multi-environment deployment</p> <p>Outputs: - <code>production_mode</code> - Pass-through for downstream jobs - Job summary with deployment mode explanation</p> <p>When it runs: First job in every ci-cd.yml run</p>"},{"location":"references/cicd/#metadata-extractyml","title":"metadata-extract.yml","text":"<p>Purpose: Extract build metadata (tags, SHA, context).</p> <p>Outputs: - Image tags (PR, SHA, latest, version) - Build context (pull_request, push, tag) - Metadata summary</p> <p>When it runs: After config job in ci-cd.yml</p>"},{"location":"references/cicd/#docker-buildyml","title":"docker-build.yml","text":"<p>Purpose: Build and push multi-platform Docker images.</p> <p>Inputs: - Image tags from metadata-extract.yml - Registry URI and location - Environment (dev/stage/prod)</p> <p>Features: - Multi-platform support (linux/amd64) - Registry cache with protected <code>buildcache</code> tag - Build provenance and SBOM generation</p> <p>Outputs: - Image digest (immutable identifier) - Digest URI (registry/image@sha256:...)</p> <p>When it runs: After metadata extraction (branch events only, not tags)</p>"},{"location":"references/cicd/#pull-and-promoteyml","title":"pull-and-promote.yml","text":"<p>Purpose: Promote images between registries (production mode only).</p> <p>Inputs: - Source environment (dev or stage) - Target environment (stage or prod) - Source digest - Target tags</p> <p>How it works: 1. Authenticate to source and target registries via WIF 2. Pull image from source registry by digest 3. Re-tag image with all target tags 4. Push to target registry</p> <p>Outputs: - Image digest (same as source) - Digest URI in target registry</p> <p>When it runs: Production mode deployments (dev \u2192 stage, stage \u2192 prod)</p>"},{"location":"references/cicd/#resolve-image-digestyml","title":"resolve-image-digest.yml","text":"<p>Purpose: Resolve image digest from tag (production mode only).</p> <p>Inputs: - Environment (stage) - Tags to resolve</p> <p>How it works: 1. Authenticate to registry via WIF 2. Query Artifact Registry for image by tag 3. Extract digest (sha256:...)</p> <p>Outputs: - Image digest - All tags associated with the image</p> <p>When it runs: Production mode tag deployments (lookup stage image for prod)</p>"},{"location":"references/cicd/#terraform-plan-applyyml","title":"terraform-plan-apply.yml","text":"<p>Purpose: Plan and apply Terraform changes.</p> <p>Inputs: - Environment (dev/stage/prod) - Action (plan/apply) - Docker image digest - WIF and state bucket details - <code>save_plan</code> (boolean) - Save plan artifact - <code>use_saved_plan</code> (boolean) - Use saved plan artifact</p> <p>Features: - Plan artifacts saved between jobs (ensures plan matches apply) - PR comment with plan output (plan-only runs) - Job summary with deployment details - Terraform format, init, validate, plan, apply steps</p> <p>When it runs: After build (or promote) for each environment</p> <p>Key behavior: - <code>plan</code> job on PR: Comment plan, don't save artifact - <code>plan</code> job on merge: Save plan artifact (no comment) - <code>apply</code> job: Use saved plan artifact</p>"},{"location":"references/cicd/#code-qualityyml","title":"code-quality.yml","text":"<p>Purpose: Run code quality checks (ruff, mypy, pytest).</p> <p>Steps: 1. Install uv and Python 3.13 2. Install dependencies with <code>uv sync --locked</code> 3. Run ruff format check 4. Run ruff linting 5. Run mypy type checking 6. Run pytest with coverage</p> <p>Timeout: 10 minutes (typical: 2-3 minutes)</p> <p>When it runs: Push to main (paths filtered) or called by required-checks.yml</p>"},{"location":"references/cicd/#required-checksyml","title":"required-checks.yml","text":"<p>Purpose: Conditional status check wrapper for branch protection.</p> <p>How it works: 1. <code>check-changes</code> job: Use paths-filter to detect code changes 2. <code>code-quality</code> job: Run if code changed 3. <code>required-status</code> job: Always run (required in branch protection)    - Pass if no code changes    - Pass if code changed and quality checks passed    - Fail if code changed and quality checks failed</p> <p>Why this exists: Branch protection requires a status check that always runs. This wrapper allows skipping quality checks when code hasn't changed while maintaining a consistent required status.</p>"},{"location":"references/cicd/#workflow-behavior","title":"Workflow Behavior","text":"<p>Build cache: - Registry cache with protected <code>buildcache</code> tag - Significant speedup on cache hits - Never expires (protected by cleanup policy in bootstrap)</p> <p>Timeouts: - Build: 30 minutes - Deploy: 20 minutes per environment - Code quality: 10 minutes</p> <p>See workflow files for specific timeout values.</p>"},{"location":"references/cicd/#job-summaries","title":"Job Summaries","text":"<p>Workflows generate formatted summaries in GitHub Actions UI:</p> <p>Config summary: - Deployment mode (dev-only vs production) - Environment deployment plan - Mode switching instructions</p> <p>Metadata extraction: - Build context (PR, main, tag, manual) - Branch/tag name and commit SHA - All image tags (bulleted list)</p> <p>Terraform deployment: - Environment and action (plan/apply) - Docker image being deployed - Step outcomes (format, init, validate, plan, apply) - Deployed resources (Cloud Run URL, Agent Engine, GCS bucket) - Collapsible plan output</p> <p>Job summaries provide quick insight without log analysis.</p>"},{"location":"references/cicd/#pr-comments","title":"PR Comments","text":"<p>Terraform plan workflow posts formatted comments on PRs:</p> <p>Comment includes: - Plan summary (resources to add/change/destroy) - Collapsible sections for detailed output - Format, init, validation results - Full plan output</p> <p>Permissions: Requires <code>pull-requests: write</code> in ci-cd.yml (configured).</p>"},{"location":"references/cicd/#authentication","title":"Authentication","text":"<p>Workload Identity Federation (WIF): - Keyless authentication (no service account keys) - GitHub Actions requests OIDC token - GCP validates against WIF provider - Grants temporary credentials scoped to repository</p> <p>IAM roles: See <code>terraform/bootstrap/module/gcp/main.tf</code> for complete role list.</p> <p>Security: - Repository-scoped IAM bindings (attribute condition on repository name) - Minimal permissions (only required roles) - Environment isolation (production mode, separate projects) - Cross-project IAM is registry-scoped (not project-level)</p>"},{"location":"references/cicd/#customization","title":"Customization","text":""},{"location":"references/cicd/#change-deployment-mode","title":"Change Deployment Mode","text":"<p>Edit <code>production_mode</code> in <code>.github/workflows/ci-cd.yml</code>:</p> <pre><code>jobs:\n  config:\n    uses: ./.github/workflows/config-summary.yml\n    with:\n      production_mode: true  # or false for dev-only\n</code></pre> <p>See Deployment Modes for complete instructions.</p>"},{"location":"references/cicd/#add-environment-variables","title":"Add Environment Variables","text":"<p>Runtime config (LOG_LEVEL, ROOT_AGENT_MODEL, etc.): 1. Settings \u2192 Environments \u2192 {environment} \u2192 Environment variables 2. Add or edit variable 3. Re-run deployment or push new commit</p> <p>Infrastructure config (CORS origins, etc.): 1. Edit <code>terraform/main/main.tf</code> 2. Create PR 3. Merge PR \u2192 deploys via CI/CD</p> <p>See Deployment Modes for runtime vs infrastructure distinction.</p>"},{"location":"references/cicd/#add-build-steps","title":"Add Build Steps","text":"<p>Edit <code>.github/workflows/ci-cd.yml</code> or reusable workflows: - Code quality checks \u2192 Edit <code>code-quality.yml</code> - Integration tests \u2192 Add job after <code>docker-build</code> in <code>ci-cd.yml</code> - Custom notifications \u2192 Add to orchestrator</p>"},{"location":"references/cicd/#modify-triggers","title":"Modify Triggers","text":"<p>Edit <code>.github/workflows/ci-cd.yml</code> triggers:</p> <pre><code>on:\n  pull_request:\n    paths:\n      - 'src/**'\n      # Add more paths\n  push:\n    branches:\n      - main\n      # Add more branches\n  push:\n    tags:\n      - 'v*'\n      # Add more tag patterns\n</code></pre> <p>\u2190 Back to References | Documentation</p>"},{"location":"references/code-quality/","title":"Code Quality","text":"<p>Detailed code quality tools, enforcement strategies, and exclusion guidelines.</p>"},{"location":"references/code-quality/#overview","title":"Overview","text":"<p>Tools to enforce code quality: 1. Ruff - Fast linting and formatting (replaces flake8, isort, black) 2. Mypy - Static type checking 3. Pytest - Testing with coverage</p> <p>All configured in <code>pyproject.toml</code>.</p>"},{"location":"references/code-quality/#running-quality-checks","title":"Running Quality Checks","text":"<p>Run before every commit: <pre><code># Use ruff to format code first (line length, quotes, whitespace)\nuv run ruff format\n\n# Then lint with ruff auto-fix (security, bugs, style violations)\nuv run ruff check --fix\n\n# Type check\nuv run mypy\n\n# Tests with coverage\nuv run pytest --cov --cov-report=term-missing\n</code></pre></p> <p>One-liner for commit: <pre><code>uv run ruff format &amp;&amp; uv run ruff check &amp;&amp; uv run mypy &amp;&amp; uv run pytest --cov\n</code></pre></p>"},{"location":"references/code-quality/#ruff-configuration","title":"Ruff Configuration","text":""},{"location":"references/code-quality/#what-we-enforce","title":"What We Enforce","text":"<p>From <code>pyproject.toml</code>: <pre><code>[tool.ruff.lint]\nselect = [\n    \"E\",   # pycodestyle errors (PEP 8 violations)\n    \"W\",   # pycodestyle warnings\n    \"F\",   # pyflakes (unused imports, undefined names)\n    \"I\",   # isort (import sorting)\n    \"B\",   # flake8-bugbear (common bugs and design problems)\n    \"C4\",  # flake8-comprehensions (better list/dict comprehensions)\n    \"UP\",  # pyupgrade (modern Python syntax)\n    \"N\",   # pep8-naming (naming conventions)\n    \"S\",   # flake8-bandit (security issues)\n    \"SIM\", # flake8-simplify (code simplification)\n    \"PTH\", # flake8-use-pathlib (use Path instead of os.path)\n]\n</code></pre></p> <p>Key enforcements: - E501: Line too long (88 characters) - PTH (pathlib rules): Use <code>pathlib.Path</code> instead of <code>os.path</code>   - PTH123: Use <code>Path.open()</code> instead of <code>open()</code>   - PTH118: Use <code>Path.joinpath()</code> instead of <code>os.path.join()</code>   - Why: Modern API, cross-platform, chainable (<code>path / \"subdir\"</code>), type-safe   - Ruff auto-fixes many os.path calls to Path - S (security rules): Catch common security issues   - S101: Use of <code>assert</code> (banned in production, allowed in tests via per-file ignore)   - S104: Binding to 0.0.0.0 (flagged for security, intentional in containers)   - S105: Possible hardcoded password (detects patterns like \"password = 'secret'\")</p>"},{"location":"references/code-quality/#per-file-exclusions","title":"Per-file Exclusions","text":"<p>From <code>pyproject.toml</code>: <pre><code>[tool.ruff.lint.per-file-ignores]\n\"tests/**/*.py\" = [\"S101\"]  # Allow assert statements in tests\n</code></pre></p> <p>Why: Tests use <code>assert</code> for validation. This is standard pytest practice and not a security concern.</p>"},{"location":"references/code-quality/#inline-exclusions","title":"Inline Exclusions","text":"<p>Real example from our codebase: <pre><code># tests/conftest.py:42\nmock_credentials.token = \"test-mock-token-totally-not-real\"  # noqa: S105\n</code></pre></p> <p>Rule: S105 (Possible hardcoded password) Why excluded: This is a test mock credential, not a real secret. The string content makes it obvious it's not production.</p> <p>Real example from tests: <pre><code># tests/test_config.py\n\"HOST\": \"0.0.0.0\",  # noqa: S104\n</code></pre></p> <p>Rule: S104 (Binding to all interfaces) Why excluded: Docker containers need to bind to 0.0.0.0 to accept connections from the host. This is intentional and documented.</p> <p>When to use <code># noqa</code>: - Security rules (S*) with false positives on test code - Line length (E501) for URLs or data that can't be split - Specific rule violation that's intentional and safe</p> <p>Prefer specific rule codes: <pre><code># Good - clear what's being suppressed\nurl = \"https://example.com/very/long/api/path\"  # noqa: E501\n\n# Avoid - unclear what's being allowed\nsome_code()  # noqa\n</code></pre></p> <p>Use <code># noqa</code> only when: - Rule violation is intentional and well-justified - Auto-fix would make code less readable - External constraint prevents compliance (e.g., third-party API format)</p>"},{"location":"references/code-quality/#ruff-auto-fix","title":"Ruff Auto-fix","text":"<p>Ruff can automatically fix many issues:</p> <pre><code># Auto-fix all fixable issues\nuv run ruff check --fix\n\n# Preview fixes without applying (safe mode)\nuv run ruff check --diff\n</code></pre> <p>What gets auto-fixed: - Import sorting (rule I) - Unused imports (rule F401) - Whitespace and indentation (rules E, W) - Quote style normalization (rules Q) - Trailing commas (rules COM) - Simple code transformations (e.g., <code>os.path.join()</code> \u2192 <code>Path.joinpath()</code>)</p> <p>What requires manual fixing: - Complex logic issues - Security concerns (rule S) - Naming violations (rule N) - Code simplification requiring logic changes (rule SIM)</p> <p>Best practice: 1. Run <code>uv run ruff format</code> first (formats code) 2. Run <code>uv run ruff check --fix</code> second (auto-fixes linting issues) 3. Review remaining issues and fix manually</p>"},{"location":"references/code-quality/#mypy-configuration","title":"Mypy Configuration","text":""},{"location":"references/code-quality/#what-we-enforce_1","title":"What We Enforce","text":"<p>From <code>pyproject.toml</code>: <pre><code>[tool.mypy]\npython_version = \"3.13\"\nmypy_path = \"src\"\npackages = [\"agent_foundation\"]\n\n# Completeness checks\ndisallow_untyped_defs = true         # All functions must have type hints\ndisallow_incomplete_defs = true      # Complete type hints (all args and return)\ndisallow_untyped_decorators = true   # Decorators must preserve types\n\n# Type system strictness\nno_implicit_optional = true          # Explicit Optional[] for None values\nstrict_equality = true               # Proper type checking in == comparisons\n\n# Warnings and errors\nwarn_return_any = true               # Flag functions returning Any\nwarn_unused_configs = true           # Flag unused config options\nwarn_redundant_casts = true          # No unnecessary casts\nwarn_unused_ignores = true           # Remove unused # type: ignore\nwarn_no_return = true                # Functions must return or raise\nwarn_unreachable = true              # Detect unreachable code\n\n# Additional strictness\ncheck_untyped_defs = true            # Check function bodies even without annotations\nshow_error_codes = true              # Show error codes for easier suppression\n</code></pre></p> <p>This is stricter than mypy's default - every function, every parameter, every return value must be typed.</p> <p>Key enforcements: - disallow_untyped_defs: Every function needs type hints (args + return) - disallow_incomplete_defs: Can't mix typed and untyped parameters - no_implicit_optional: <code>def foo(x: str = None)</code> is an error (must use <code>str | None</code>) - warn_return_any: Flag when functions return <code>Any</code> (type safety leak) - strict_equality: <code>if my_str == 5:</code> is an error (comparing incompatible types) - warn_unreachable: Detect dead code after returns/raises</p>"},{"location":"references/code-quality/#example-errors-and-fixes","title":"Example Errors and Fixes","text":"<p>Error: disallow_untyped_defs <pre><code># mypy error: Function is missing a return type annotation\ndef process_data(items: list[str]):\n    return len(items)\n\n# Fixed\ndef process_data(items: list[str]) -&gt; int:\n    return len(items)\n</code></pre></p> <p>Error: disallow_incomplete_defs <pre><code># mypy error: Function is missing a type annotation for one or more arguments\ndef fetch_user(id: int, cache):\n    return cache.get(id)\n\n# Fixed\ndef fetch_user(id: int, cache: dict[int, User]) -&gt; User | None:\n    return cache.get(id)\n</code></pre></p> <p>Error: no_implicit_optional <pre><code># mypy error: Incompatible default for argument (expected str, got None)\ndef greet(name: str = None) -&gt; str:\n    return f\"Hello, {name or 'stranger'}\"\n\n# Fixed - explicit None in union\ndef greet(name: str | None = None) -&gt; str:\n    return f\"Hello, {name or 'stranger'}\"\n</code></pre></p>"},{"location":"references/code-quality/#per-module-exclusions","title":"Per-module Exclusions","text":"<p>From <code>pyproject.toml</code>: <pre><code>[[tool.mypy.overrides]]\nmodule = \"tests.*\"\ndisable_error_code = \"arg-type\"  # Allow duck-typed mocks in tests\n</code></pre></p> <p>Why: Test files can pass duck-typed mocks to functions without arg-type errors, enabling pragmatic testing patterns.</p> <p>Important caveat: We still strictly type <code>conftest.py</code> fixture definitions as a team practice (see Team Practices section for rationale).</p>"},{"location":"references/code-quality/#inline-exclusions_1","title":"Inline Exclusions","text":"<p>When you might need <code># type: ignore</code>: <pre><code># Third-party library without type stubs\nimport untyped_library  # type: ignore[import-untyped]\n\n# Mypy bug or limitation (add TODO)\nresult = complex_function()  # type: ignore[return-value]  # TODO: Fix when mypy supports this pattern\n</code></pre></p> <p>Always prefer: 1. Fix the actual type issue (restructure code to make types clear) 2. Add type stubs for third-party libraries 3. Use type narrowing - validate types at runtime with <code>isinstance()</code> checks or type guards</p> <p>Never use <code>cast()</code> - use type narrowing instead:</p> <p>Why <code>cast()</code> is bad: - Bypasses type safety: Tells mypy to trust you without verification - Runtime risk: No runtime validation - type mismatch crashes at runtime - Dishonest about code behavior: Hides the actual types being used - Maintenance burden: Future changes break assumptions silently</p> <p>Type narrowing is always better: <pre><code># Bad - using cast() (DON'T DO THIS)\nfrom typing import cast\nresult = some_function()\nvalue = cast(str, result)  # Trust me, it's a string! (no runtime check)\n\n# Good - type narrowing with isinstance()\nresult = some_function()\nif not isinstance(result, str):\n    raise TypeError(f\"Expected str, got {type(result)}\")\nvalue = result  # mypy knows this is str now, AND we validated at runtime\n</code></pre></p> <p>Type narrowing keeps you honest about what the code actually does - it satisfies both mypy and provides runtime safety.</p>"},{"location":"references/code-quality/#coverage-configuration","title":"Coverage Configuration","text":""},{"location":"references/code-quality/#what-we-enforce_2","title":"What We Enforce","text":"<p>From <code>pyproject.toml</code>: <pre><code>[tool.coverage.run]\nsource = [\"src\"]\nbranch = true  # Check both line and branch coverage\n\n[tool.coverage.report]\nfail_under = 100  # Require 100% coverage\n</code></pre></p> <p>100% coverage is non-negotiable for production code.</p>"},{"location":"references/code-quality/#file-level-exclusions","title":"File-level Exclusions","text":"<p>From <code>pyproject.toml</code>: <pre><code>[tool.coverage.run]\nomit = [\n    # __init__.py files: namespace marker, no logic\n    \"src/agent_foundation/**/__init__.py\",\n\n    # server.py: FastAPI setup and ADK initialization (integration tested via CI/CD)\n    \"src/agent_foundation/server.py\",\n\n    # agent.py: LlmAgent instantiation (tested through callbacks, not in isolation)\n    \"src/agent_foundation/**/agent.py\",\n\n    # prompt.py: Prompt text and formatting (integration tested via agent runs)\n    \"src/agent_foundation/**/prompt.py\",\n\n    # observability.py: OpenTelemetry setup (infrastructure initialization)\n    \"src/agent_foundation/utils/observability.py\",\n]\n</code></pre></p> <p>Rationale for each: - <code>__init__.py</code>: Pure namespace markers with no logic - <code>server.py</code>: FastAPI entrypoint with ADK initialization - tested via integration tests in CI/CD - <code>agent.py</code>: Pure ADK configuration (plugin registration, model selection) - tested through agent behavior - <code>prompt.py</code>: Prompt templates and formatting - tested via agent evaluations, not unit tests - <code>observability.py</code>: OpenTelemetry infrastructure setup - validated through trace output, not unit tests</p> <p>Pattern: We exclude configuration and infrastructure initialization files. Business logic and utilities must have 100% coverage.</p>"},{"location":"references/code-quality/#inline-exclusions_2","title":"Inline Exclusions","text":"<p>Real example from our codebase: <pre><code># src/agent_foundation/utils/config.py:234\nif not isinstance(result, list):  # pragma: no cover\n    # Pydantic validation makes this unreachable\n    msg = \"Invalid allow_origins format\"\n    raise TypeError(msg)\n</code></pre></p> <p>Why: The <code>@field_validator</code> above this property ensures <code>result</code> is always a list. This defensive check exists for static type safety but is unreachable at runtime. Pydantic guarantees it.</p> <p>When to use <code># pragma: no cover</code>: - Defensive code (provably unreachable) needed to satisfy the static type-checker - Platform-specific branches not testable in CI (e.g., Windows-only code) - Error paths that require external system failures (rare)</p> <p>When NOT to use it: - \"This is hard to test\" - write better tests or use better code patterns to facilitate testing - \"Coverage is annoying\" - coverage is finding gaps as intended - Normal error handling - always test error paths</p>"},{"location":"references/code-quality/#workflow-integration","title":"Workflow Integration","text":""},{"location":"references/code-quality/#when-to-run-checks","title":"When to Run Checks","text":"<p>Before every commit: <pre><code>uv run ruff format &amp;&amp; uv run ruff check &amp;&amp; uv run mypy &amp;&amp; uv run pytest --cov\n</code></pre></p> <p>During development: - Run <code>ruff format</code> and <code>ruff check --fix</code> frequently as you code - Run mypy after adding new functions or changing signatures - Run tests after behavior changes</p> <p>IDE Integration: - Configure your editor to run ruff format on save - Enable mypy real-time checking for immediate feedback - Use pytest plugin for test execution and debugging</p> <p>CI Enforcement: All checks run automatically in GitHub Actions on every PR: - Code quality workflow runs ruff, mypy, pytest - Blocks merge if any check fails - Ensures main branch always passes quality gates</p>"},{"location":"references/code-quality/#team-practices","title":"Team Practices","text":"<p>Test Suite Typing Strategy:</p> <p>We disable mypy <code>arg-type</code> checking for test files, but maintain strict discipline in <code>conftest.py</code>:</p> <pre><code># conftest.py - strictly typed even though mypy doesn't enforce it\n@pytest.fixture\ndef mock_state() -&gt; MockState:\n    \"\"\"Create a mock state with test data.\"\"\"\n    return MockState({\"user_id\": \"user123\", \"session_data\": {\"key\": \"value\"}})\n\n@pytest.fixture\ndef mock_content() -&gt; MockContent:\n    \"\"\"Create a mock content with test data.\"\"\"\n    return MockContent({\"text\": \"Hello, agent!\"})\n</code></pre> <p>Why this works: - Test files can pass duck-typed mocks without mypy errors (pragmatic testing) - <code>conftest.py</code> fixtures have clear type contracts (self-documenting, IDE support) - Best of both worlds: strict where it matters, flexible where it helps</p> <p>This is a team practice, not enforced by tooling. Maintain discipline when writing fixtures.</p>"},{"location":"references/code-quality/#philosophy","title":"Philosophy","text":""},{"location":"references/code-quality/#why-these-standards","title":"Why These Standards?","text":"<p>Strict typing (mypy): - Catch bugs before runtime - Enable powerful IDE features - Document code contracts - Facilitate safe refactoring</p> <p>100% coverage: - Confidence in changes - Forces thinking about edge cases - Documents all code paths - Prevents untested code accumulation</p> <p>Automated formatting (ruff): - Eliminates style debates - Consistent codebase - Fast code review - Auto-fix reduces friction</p> <p>Security checks (bandit via ruff): - Catch common security mistakes - Enforce safe defaults - Educate developers on risks</p>"},{"location":"references/code-quality/#when-to-add-exclusions","title":"When to Add Exclusions","text":"<p>Never add exclusions: - To make broken code pass checks - Because it's \"too much work\" - For personal style preference - To ship faster without fixing issues</p> <p>Acceptable exclusions: - Third-party library limitations (document why, add TODO if fixable) - Tool bugs (add TODO to fix when tool updates) - Provably unreachable defensive code (rare, requires strong justification) - Test-specific patterns (e.g., assert in tests, mock credentials)</p> <p>Best practices: 1. Be specific: Use rule codes (<code># noqa: S105</code>) not blanket suppression (<code># noqa</code>) 2. Document why: Add comment explaining the exclusion 3. Add TODOs: If it's a workaround, plan to fix it later 4. Review regularly: Remove exclusions when underlying issue is fixed</p> <p>Always prefer fixing the issue over adding exclusions.</p> <p>\u2190 Back to References | Documentation</p>"},{"location":"references/deployment/","title":"Deployment Modes Reference","text":"<p>Multi-environment deployment strategy, infrastructure parity, and image promotion.</p>"},{"location":"references/deployment/#deployment-mode-comparison","title":"Deployment Mode Comparison","text":""},{"location":"references/deployment/#dev-only-mode-default","title":"Dev-Only Mode (Default)","text":"<p>Infrastructure: - Single GCP project - Single Terraform deployment environment (dev) - One GitHub Environment variable scope - No tag protection</p> <p>Workflow: - PR \u2192 dev plan (comment on PR) - Merge \u2192 dev deploy - Tag \u2192 dev deploy (version labeled)</p> <p>Use cases: - Experiments and prototypes - Internal tools with limited user base - Cost optimization (single project) - Rapid iteration without approval gates</p> <p>Configuration: <pre><code># .github/workflows/ci-cd.yml\njobs:\n  config:\n    uses: ./.github/workflows/config-summary.yml\n    with:\n      production_mode: false\n</code></pre></p>"},{"location":"references/deployment/#production-mode-opt-in","title":"Production Mode (Opt-In)","text":"<p>Infrastructure: - Three GCP projects (dev/stage/prod) - Four GitHub Environments (dev/stage/prod/prod-apply) - Environment-scoped variables - Tag protection ruleset (v*)</p> <p>Workflow: - PR \u2192 dev plan (comment on PR) - Merge \u2192 dev + stage deploy (parallel) - Tag \u2192 prod deploy (manual approval required)</p> <p>Use cases: - Customer-facing production services - Compliance requiring staged deployment - Infrastructure validation before production - Rollback capability critical</p> <p>Configuration: <pre><code># .github/workflows/ci-cd.yml\njobs:\n  config:\n    uses: ./.github/workflows/config-summary.yml\n    with:\n      production_mode: true\n</code></pre></p>"},{"location":"references/deployment/#switching-modes","title":"Switching Modes","text":"<p>Requirements: 1. Bootstrap Github Environment(s) and Google Cloud project(s) for target mode 2. Configure protection rules (if switching to production mode) 3. Create PR with mode change in ci-cd.yml 4. Merge PR to apply new workflow behavior</p> <p>Why config job parameter? GitHub Actions doesn't allow accessing workflow-level <code>env</code> variables (like <code>env.PRODUCTION_MODE: {true/false}</code>)in jobs that call reusable workflows. The config job output pattern works around this limitation.</p>"},{"location":"references/deployment/#multi-environment-strategy","title":"Multi-Environment Strategy","text":""},{"location":"references/deployment/#infrastructure-parity","title":"Infrastructure Parity","text":"<p>All environments use identical infrastructure configuration. Stage validates the exact infrastructure that will deploy to prod.</p> <p>Differences between environments: - Resource names (via <code>environment</code> variable: dev/stage/prod) - Runtime app config (GitHub Environment variables: LOG_LEVEL, etc.) - Cleanup policies (configured per environment in bootstrap)</p> <p>Infrastructure config is hard-coded in Terraform and identical across environments. This ensures infrastructure changes (for example, the Cloud Run service minimum instance count) require explicit file edits and PR review, not hidden variable overrides.</p>"},{"location":"references/deployment/#deployment-flow","title":"Deployment Flow","text":""},{"location":"references/deployment/#dev-only-mode","title":"Dev-Only Mode","text":"<p>Pull Request: <pre><code>build (push to dev registry)\n  \u2193\ndev-plan (plan only, PR comment)\n</code></pre></p> <p>Merge to main: <pre><code>build (push to dev registry)\n  \u2193\ndev-plan (auto, saves tfplan-dev)\n  \u2193\ndev-apply (auto-proceeds, uses saved plan)\n</code></pre></p> <p>Tag push: No effect on deployment in dev-only mode</p>"},{"location":"references/deployment/#production-mode","title":"Production Mode","text":"<p>Pull Request: <pre><code>build (push to dev registry)\n  \u2193\ndev-plan (plan only, PR comment)\n</code></pre></p> <p>Merge to main: <pre><code>build (push to dev registry)\n  \u2193\n  \u251c\u2500\u2192 dev-plan (auto, saves tfplan-dev)\n  \u2502     \u2193\n  \u2502   dev-apply (auto-proceeds, uses saved plan)\n  \u2502\n  \u2514\u2500\u2192 stage-promote (pull from dev \u2192 push to stage)\n        \u2193\n      stage-plan (auto, saves tfplan-stage)\n        \u2193\n      stage-apply (auto-proceeds, uses saved plan)\n</code></pre></p> <p>Git tag push: <pre><code>resolve-digest (look up image in stage registry by tag)\n  \u2193\nprod-promote (pull from stage \u2192 push to prod)\n  \u2193\nprod-plan (auto, saves tfplan-prod)\n  \u2193\nprod-apply (gated: requires manual approval, uses saved plan)\n</code></pre></p> <p>Key principles: - Dev deployment never waits for stage or prod - Stage validates every merge (continuous feedback) - Prod deploys only on explicit git tags (release discipline) - Uniform plan \u2192 apply pattern across all environments</p>"},{"location":"references/deployment/#image-promotion","title":"Image Promotion","text":"<p>Production mode uses image promotion (pull from source, push to target) instead of rebuilding.</p>"},{"location":"references/deployment/#dev-stage","title":"Dev \u2192 Stage","text":"<p>Trigger: Merge to main</p> <p>Process: 1. Build job pushes image to dev registry: <code>us-central1-docker.pkg.dev/dev-project/agent-dev/image@sha256:abc123</code> 2. stage-promote job:    - Authenticates to dev and stage registries via WIF    - Pulls image from dev registry by digest    - Re-tags with all source tags    - Pushes to stage registry: <code>us-central1-docker.pkg.dev/stage-project/agent-stage/image@sha256:abc123</code></p> <p>Cross-project IAM: - Stage WIF principal has <code>roles/artifactregistry.reader</code> on dev registry - Configured via <code>promotion_source_*</code> variables in stage bootstrap</p>"},{"location":"references/deployment/#stage-prod","title":"Stage \u2192 Prod","text":"<p>Trigger: Git tag push (e.g., <code>v1.0.0</code>)</p> <p>Process: 1. resolve-digest job:    - Queries stage registry for image by tag: <code>v1.0.0</code>    - Extracts digest: <code>sha256:abc123</code> 2. prod-promote job:    - Authenticates to stage and prod registries via WIF    - Pulls image from stage registry by digest    - Re-tags with all source tags    - Pushes to prod registry: <code>us-central1-docker.pkg.dev/prod-project/agent-prod/image@sha256:abc123</code></p> <p>Cross-project IAM: - Prod WIF principal has <code>roles/artifactregistry.reader</code> on stage registry - Configured via <code>promotion_source_*</code> variables in prod bootstrap</p>"},{"location":"references/deployment/#why-promote-instead-of-rebuild","title":"Why Promote Instead of Rebuild?","text":"<p>Guarantees: - Deploy the exact bytes that were tested in previous environment - No build-time differences (dependencies, base images, timestamps) - Immutable artifacts (can't accidentally rebuild with different code)</p> <p>Performance: - Faster than rebuild (just pull/tag/push) - No dependency resolution, no layer builds</p> <p>Consistency: - Same image digest across all environments - Easy to trace: \"prod is running the same image validated in stage\"</p>"},{"location":"references/deployment/#runtime-configuration","title":"Runtime Configuration","text":""},{"location":"references/deployment/#runtime-vs-infrastructure-config","title":"Runtime vs Infrastructure Config","text":"<p>Runtime app config (configurable via GitHub Environment variables): - <code>LOG_LEVEL</code> - Logging verbosity - <code>ROOT_AGENT_MODEL</code> - Gemini model selection - <code>SERVE_WEB_INTERFACE</code> - Enable web UI - <code>OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT</code> - Capture LLM content in traces</p> <p>GitHub Environment Variables pass overrides to Terraform as <code>TF_VAR_*</code> inputs. Default values managed in Terraform.</p> <p>Infrastructure config (managed exclusively in Terraform files): - <code>AGENT_ENGINE</code> - Vertex AI Agent Engine ID (auto-created) - <code>ARTIFACT_SERVICE_URI</code> - GCS bucket URL (auto-created) - <code>ALLOW_ORIGINS</code> - CORS origins for Cloud Run - Terraform-managed values only (no variable overrides)</p> <p>Why separate them: - Runtime config changes don't require Terraform rebuilds (fast iteration) - Infrastructure changes require explicit code review and PR (security) - CORS origins not overridable by GitHub variables (prevent bypass)</p>"},{"location":"references/deployment/#changing-runtime-config","title":"Changing Runtime Config","text":"<p>Update GitHub Environment variables (no code changes):</p> <ol> <li>Settings \u2192 Environments \u2192 {environment} \u2192 Environment variables</li> <li>Edit or add unset variable (e.g., <code>LOG_LEVEL=DEBUG</code> or <code>SERVE_WEB_INTERFACE=TRUE</code>)</li> <li>Re-run latest workflow or push new commit</li> </ol> <p>Changes apply on next deployment.</p>"},{"location":"references/deployment/#changing-infrastructure-config","title":"Changing Infrastructure Config","text":"<p>Edit Terraform files and create PR:</p> <pre><code>git checkout -b fix/update-cors-origins\n# Edit terraform/main/main.tf (e.g., update ALLOW_ORIGINS list)\ngit commit -m \"fix: update CORS origins\"\ngit push origin fix/update-cors-origins\ngh pr create\n# Review plan in PR comment\n# Merge PR \u2192 deploys to dev (+ stage in production mode)\n</code></pre>"},{"location":"references/deployment/#terraform-structure","title":"Terraform Structure","text":""},{"location":"references/deployment/#bootstrap-module","title":"Bootstrap Module","text":"<p>Purpose: One-time CI/CD infrastructure setup (per environment)</p> <p>Location: <code>terraform/bootstrap/{dev,stage,prod}/</code></p> <p>Resources created: - Workload Identity Federation (keyless GitHub Actions auth) - Artifact Registry (Docker image storage with cleanup policies) - Terraform State Bucket (remote state for main module) - GitHub Environments (dev/stage/prod/prod-apply in production mode) - GitHub Environment Variables (auto-configured per environment) - Tag Protection (production tag ruleset, prod bootstrap only) - Cross-Project IAM (Artifact Registry reader for promotion, stage/prod)</p> <p>State management: Local state (per environment)</p> <p>Runs: Manually by infrastructure owners (one-time setup)</p>"},{"location":"references/deployment/#main-module","title":"Main Module","text":"<p>Purpose: Application deployment (runs in CI/CD)</p> <p>Location: <code>terraform/main/</code></p> <p>Resources created: - Cloud Run Service (containerized agent deployment) - Service Account (IAM identity for Cloud Run) - Vertex AI Agent Engine (session/memory persistence) - GCS Bucket (artifact storage)</p> <p>State management: Remote state in GCS (bucket created by bootstrap)</p> <p>Runs: Automatically in GitHub Actions on merge/tag</p> <p>Inputs: All via <code>TF_VAR_*</code> environment variables from GitHub</p>"},{"location":"references/deployment/#resource-naming","title":"Resource Naming","text":"<p>All resources named: <code>${var.agent_name}-${var.environment}</code></p> <p>Examples: - Cloud Run service: <code>my-agent-dev</code>, <code>my-agent-stage</code>, <code>my-agent-prod</code> - Artifact Registry: <code>my-agent-dev</code>, <code>my-agent-stage</code>, <code>my-agent-prod</code> - Service account: <code>my-agent-dev@project.iam.gserviceaccount.com</code></p> <p>Note: Service account IDs truncate agent_name to 30 chars (GCP limit).</p> <p>\u2190 Back to References | Documentation</p>"},{"location":"references/docker-compose-workflow/","title":"Docker Compose Local Development Workflow","text":"<p>This guide covers the recommended workflow for local development using Docker Compose.</p>"},{"location":"references/docker-compose-workflow/#quick-start","title":"Quick Start","text":""},{"location":"references/docker-compose-workflow/#daily-development-recommended","title":"Daily Development (Recommended)","text":"<pre><code>docker compose up --build --watch\n</code></pre> <p>Why both flags? - <code>--build</code>: Ensures you have the latest code and dependencies - <code>--watch</code>: Enables hot reloading for instant feedback</p> <p>What happens: - Container starts with your latest code - Watch mode monitors your files for changes - Edits to <code>src/</code> files are synced instantly (no rebuild needed) - Changes to <code>pyproject.toml</code> or <code>uv.lock</code> trigger automatic rebuild</p> <p>Leave it running while you develop - changes are applied automatically!</p>"},{"location":"references/docker-compose-workflow/#common-commands","title":"Common Commands","text":""},{"location":"references/docker-compose-workflow/#start-with-hot-reloading-default-workflow","title":"Start with hot reloading (default workflow)","text":"<pre><code>docker compose up --build --watch\n</code></pre>"},{"location":"references/docker-compose-workflow/#stop-the-service","title":"Stop the service","text":"<pre><code># Press Ctrl+C to gracefully stop\n# Or in another terminal:\ndocker compose down\n</code></pre>"},{"location":"references/docker-compose-workflow/#view-logs","title":"View logs","text":"<pre><code># If running in detached mode\ndocker compose logs -f\n\n# View just the app logs\ndocker compose logs -f app\n</code></pre>"},{"location":"references/docker-compose-workflow/#rebuild-without-starting","title":"Rebuild without starting","text":"<pre><code>docker compose build\n</code></pre>"},{"location":"references/docker-compose-workflow/#run-without-watch-mode","title":"Run without watch mode","text":"<pre><code>docker compose up --build\n</code></pre>"},{"location":"references/docker-compose-workflow/#how-watch-mode-works","title":"How Watch Mode Works","text":"<p>Watch mode uses the configuration in <code>docker-compose.yml</code>:</p> <pre><code>develop:\n  watch:\n    # Sync: Instant file copy, no rebuild\n    - action: sync\n      path: ./src\n      target: /app/src\n\n    # Rebuild: Triggers full image rebuild\n    - action: rebuild\n      path: ./pyproject.toml\n\n    - action: rebuild\n      path: ./uv.lock\n</code></pre>"},{"location":"references/docker-compose-workflow/#sync-action","title":"Sync Action","text":"<ul> <li>Triggers when: You edit files in <code>src/</code></li> <li>What happens: Files are copied into running container instantly</li> <li>Speed: Immediate (no rebuild)</li> <li>Use case: Code changes during development</li> </ul>"},{"location":"references/docker-compose-workflow/#rebuild-action","title":"Rebuild Action","text":"<ul> <li>Triggers when: You edit <code>pyproject.toml</code> or <code>uv.lock</code></li> <li>What happens: Full image rebuild, container recreated</li> <li>Speed: ~5-10 seconds (with cache)</li> <li>Use case: Dependency changes</li> </ul>"},{"location":"references/docker-compose-workflow/#file-locations","title":"File Locations","text":""},{"location":"references/docker-compose-workflow/#source-code","title":"Source Code","text":"<ul> <li>Host: <code>./src/</code></li> <li>Container: <code>/app/src</code></li> <li>Sync: Automatic via watch mode</li> </ul>"},{"location":"references/docker-compose-workflow/#credentials","title":"Credentials","text":"<ul> <li>Host: <code>~/.config/gcloud/</code></li> <li>Container: <code>/gcloud/</code></li> <li>Mount: Read-only volume</li> <li>Purpose: Secure access for the local development to Application Default Credentials for Google authentication</li> </ul>"},{"location":"references/docker-compose-workflow/#data-directory","title":"Data Directory","text":"<ul> <li>Host: <code>./data/</code></li> <li>Container: <code>/app/data</code> (read-only)</li> <li>Purpose: Optional data files for agent</li> </ul>"},{"location":"references/docker-compose-workflow/#environment-variables","title":"Environment Variables","text":"<p>Docker Compose loads <code>.env</code> automatically. See Environment Variables Guide for details on required and optional variables.</p> <p>Note: The container uses <code>HOST=0.0.0.0</code> to allow connections from the host machine.</p>"},{"location":"references/docker-compose-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"references/docker-compose-workflow/#container-keeps-restarting","title":"Container keeps restarting","text":"<ul> <li>Check logs: <code>docker compose logs -f</code></li> <li>Verify <code>.env</code> file exists and has required variables</li> <li>Ensure Application Default Credentials are configured: <code>gcloud auth application-default login</code></li> </ul>"},{"location":"references/docker-compose-workflow/#changes-not-appearing","title":"Changes not appearing","text":"<ul> <li>For code changes: Should sync instantly via watch mode</li> <li>For dependency changes: Watch should auto-rebuild</li> <li>If stuck: Stop and restart with <code>docker compose up --build --watch</code></li> </ul>"},{"location":"references/docker-compose-workflow/#permission-errors","title":"Permission errors","text":"<ul> <li>Data directory: Mounted read-only, should not need write access</li> <li>Credentials: Ensure <code>~/.config/gcloud/application_default_credentials.json</code> exists and is readable</li> </ul>"},{"location":"references/docker-compose-workflow/#port-already-in-use","title":"Port already in use","text":"<pre><code># Check what's using port 8000\nlsof -i :8000\n\n# Stop the conflicting process or change PORT in .env\nPORT=8001\n</code></pre>"},{"location":"references/docker-compose-workflow/#windows-path-compatibility","title":"Windows path compatibility","text":"<ul> <li>The <code>docker-compose.yml</code> uses <code>${HOME}</code> which is Unix/Mac specific</li> <li>Windows users need to update the volume path in <code>docker-compose.yml</code>:</li> <li>Replace <code>${HOME}/.config/gcloud/application_default_credentials.json</code></li> <li>With your Windows path: <code>C:\\Users\\YourUsername\\AppData\\Roaming\\gcloud\\application_default_credentials.json</code></li> <li>Alternative: Use <code>%USERPROFILE%</code> environment variable in PowerShell</li> <li>See the comment in <code>docker-compose.yml</code> for the exact syntax</li> </ul>"},{"location":"references/docker-compose-workflow/#testing-registry-images","title":"Testing Registry Images","text":"<p>For rare cases when you need to test the exact image from CI/CD:</p> <pre><code># Authenticate once\ngcloud auth configure-docker us-central1-docker.pkg.dev\n\n# Set your image\nexport REGISTRY_IMAGE=\"us-central1-docker.pkg.dev/project/repo/app:sha123\"\n\n# Pull and run with docker-compose\ndocker pull $REGISTRY_IMAGE\ndocker compose run -e IMAGE=$REGISTRY_IMAGE app\n</code></pre> <p>Alternative - direct run: <pre><code>docker run --rm \\\n  -v ./data:/app/data:ro \\\n  -v ~/.config/gcloud/application_default_credentials.json:/gcloud/application_default_credentials.json:ro \\\n  -e GOOGLE_APPLICATION_CREDENTIALS=/gcloud/application_default_credentials.json \\\n  -p 127.0.0.1:8000:8000 \\\n  --env-file .env \\\n  $REGISTRY_IMAGE\n</code></pre></p>"},{"location":"references/docker-compose-workflow/#direct-docker-commands-without-compose","title":"Direct Docker Commands (Without Compose)","text":"<p>If you need to build and run without docker-compose:</p> <pre><code># Build the image with BuildKit\nDOCKER_BUILDKIT=1 docker build -t your-agent-name:latest .\n\n# Run directly\ndocker run \\\n  -v ./data:/app/data:ro \\\n  -p 127.0.0.1:8000:8000 \\\n  --env-file .env \\\n  your-agent-name:latest\n</code></pre> <p>Note: Docker Compose is recommended - it handles volumes, environment, and networking automatically.</p>"},{"location":"references/docker-compose-workflow/#references","title":"References","text":"<ul> <li>Docker Compose Documentation</li> <li>Docker Compose Watch Mode</li> <li>Dockerfile Strategy Guide - Architecture decisions and design rationale</li> </ul> <p>\u2190 Back to References | Documentation</p>"},{"location":"references/dockerfile-strategy/","title":"Dockerfile Strategy Explained","text":"<p>This document explains our multi-stage Docker build strategy and why we chose this approach for containerizing the agent.</p>"},{"location":"references/dockerfile-strategy/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Dockerfile Breakdown</li> <li>Why Not Use UV Image Directly</li> <li>Performance Comparison</li> <li>Summary</li> </ul>"},{"location":"references/dockerfile-strategy/#overview","title":"Overview","text":"<p>Our Dockerfile uses a multi-stage build with the following architecture:</p> <ol> <li>Builder Stage: <code>python:3.13-slim</code> + uv binary (copied from Astral's distroless image)</li> <li>Runtime Stage: Clean <code>python:3.13-slim</code> + only the virtual environment</li> </ol> <p>This approach gives us: - \u2705 Official uv binary (always latest) - \u2705 Full build capabilities (shell, Python, package manager) - \u2705 Minimal runtime image (~200MB vs ~500MB) - \u2705 Fast rebuilds (5-10s for code changes) - \u2705 Maximum security (non-root, minimal attack surface)</p>"},{"location":"references/dockerfile-strategy/#dockerfile-breakdown","title":"Dockerfile Breakdown","text":""},{"location":"references/dockerfile-strategy/#buildkit-directive","title":"BuildKit Directive","text":"<p><pre><code># syntax=docker/dockerfile:1\n</code></pre> What: Tells Docker to use BuildKit parser (modern Docker build engine) Why: Enables advanced features like <code>--mount=type=cache</code> and parallel builds</p>"},{"location":"references/dockerfile-strategy/#builder-stage-base-image","title":"Builder Stage Base Image","text":"<p><pre><code>FROM python:3.13-slim AS builder\n</code></pre> What: Start builder stage with official Python 3.13 slim image (Debian-based) Why: We need: - Python runtime for <code>uv sync</code> to work - Shell and basic utilities (cp, mkdir, etc.) for build commands</p> <p>Why not <code>ghcr.io/astral-sh/uv:latest</code> as base? - Astral's uv image is distroless (no shell, no package manager) - You can't run <code>RUN</code> commands in distroless images - It's designed to copy the binary FROM, not build FROM</p>"},{"location":"references/dockerfile-strategy/#copy-uv-binary","title":"Copy UV Binary","text":"<p><pre><code>COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\n</code></pre> What: Extract just the <code>uv</code> and <code>uvx</code> binaries from Astral's image Why: - Gets latest uv version without manually tracking releases - Copies only ~10MB of binaries (not a whole base image) - Puts them in <code>/bin/</code> so they're in PATH</p> <p>Key insight: We get official uv without the distroless constraints.</p>"},{"location":"references/dockerfile-strategy/#working-directory-builder-stage","title":"Working Directory (Builder Stage)","text":"<p><pre><code>WORKDIR /app\n</code></pre> What: Set working directory to <code>/app</code> Why: - All subsequent commands run from this directory - Creates the directory if it doesn't exist - Standard convention for application code</p>"},{"location":"references/dockerfile-strategy/#uv-environment-variables","title":"UV Environment Variables","text":"<p><pre><code>ENV UV_LINK_MODE=copy \\\n    UV_COMPILE_BYTECODE=1 \\\n    UV_PYTHON_DOWNLOADS=never\n</code></pre> What: Configure uv behavior Why:</p> Variable Value Reason <code>UV_LINK_MODE=copy</code> Copy files instead of hardlinking Safer for Docker layers, works across filesystems <code>UV_COMPILE_BYTECODE=1</code> Pre-compile .py \u2192 .pyc files Faster startup (no compilation at runtime) <code>UV_PYTHON_DOWNLOADS=never</code> Don't download Python Use system Python from base image"},{"location":"references/dockerfile-strategy/#install-dependencies","title":"Install Dependencies","text":"<p><pre><code># Copy dependency files - explicit cache invalidation when either file changes\nCOPY pyproject.toml uv.lock ./\n\n# Install dependencies (cache mount provides the performance optimization)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-install-project --no-dev\n</code></pre> What: Install dependencies WITHOUT installing the project itself Why: Key optimization strategy</p> Flag Meaning Benefit <code>--mount=type=cache</code> Persist <code>/root/.cache/uv</code> across builds Don't re-download packages if already cached (~80% speedup) <code>--locked</code> Validate lockfile matches pyproject.toml Catches mistakes, prevents silent failures (our standard) <code>--no-install-project</code> Skip installing <code>src/may_package</code> Separates dependencies (slow) from code (fast) <code>--no-dev</code> Skip dev dependencies Smaller image <p>Why COPY both files? - \u2705 Explicit cache invalidation - When dependencies change, this layer rebuilds (as it should) - \u2705 Predictable - Standard Docker caching behavior, no surprises - \u2705 Simple - Easy to understand and maintain - \u2705 Fast anyway - Cache mount makes rebuilds quick (~2-5s even on version bumps)</p> <p>Why <code>--locked</code> (not <code>--frozen</code>)?</p> <p>We standardize on <code>--locked</code> for all builds. Here's why:</p> Flag Behavior When to Use <code>--locked</code> Validates lockfile matches pyproject.toml \u2705 Always (our standard) <code>--frozen</code> Skips validation, silently uses stale lockfile \u274c Avoid <p>Why avoid <code>--frozen</code>: - \u274c Silently installs wrong dependencies if lockfile is stale - \u274c Even UV's official CI/CD examples use <code>--locked</code> - \u274c Hides developer mistakes instead of catching them</p> <p>Why use <code>--locked</code> everywhere: - \u2705 Catches developer mistakes (forgot to run <code>uv lock</code>) - \u2705 Ensures lockfile and pyproject.toml stay synchronized - \u2705 Fails fast with clear error message - \u2705 Validation is negligible cost (~milliseconds) - \u2705 Enforces correct workflow: change deps \u2192 <code>uv lock</code> \u2192 commit both files</p> <p>Example of <code>--locked</code> preventing bugs: <pre><code># Developer adds pandas to pyproject.toml but forgets to run uv lock\n$ docker build .\nERROR: The lockfile is out of sync with pyproject.toml\n# Developer: \"Oh right, I need to run uv lock first!\"\n$ uv lock\n$ docker build .  # Now succeeds with correct dependencies\n</code></pre></p> <p>Bottom line: We use <code>--locked</code> in development, CI/CD, and production. We'll only consider <code>--frozen</code> if we encounter a very specific use case that requires skipping validation (none identified yet).</p> <p>Cache mount is the real optimization - It persists across builds, so even \"unnecessary\" rebuilds are fast.</p>"},{"location":"references/dockerfile-strategy/#copy-source-code","title":"Copy Source Code","text":"<p><pre><code># Copy only source code (documentation changes won't invalidate this layer)\nCOPY src ./src\n</code></pre> What: Copy only the application source code Why: - Done AFTER dependencies to maximize cache hits - Optimization: Only copies source code, not documentation (README.md) - Documentation updates won't invalidate this layer or trigger project reinstall - Code changes trigger rebuild as expected - This layer rebuilds only on source code changes (~5-10s) - More targeted than <code>COPY . /app</code> - avoids redundant pyproject.toml/uv.lock copy</p>"},{"location":"references/dockerfile-strategy/#install-project","title":"Install Project","text":"<p><pre><code># Install project itself (create empty README to satisfy package metadata requirements)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    touch README.md &amp;&amp; \\\n    uv sync --locked --no-editable --no-dev\n</code></pre> What: Install the project itself (now that source code exists) Why: - <code>touch README.md</code>: Creates empty README to satisfy package metadata requirements   - Performance optimization: README changes won't trigger this layer rebuild   - Only source code changes (<code>src/</code>) invalidate this layer   - Intentional trade-off: runtime container won't have real README (not needed for execution) - <code>--locked</code>: Validates lockfile matches pyproject.toml (catches mistakes) - <code>--no-editable</code>: Install as a regular package (not editable/develop mode) - Reuses dependencies from previous step (already in .venv) - Fast because dependencies already installed (~5-10s)</p>"},{"location":"references/dockerfile-strategy/#runtime-stage-base","title":"Runtime Stage Base","text":"<p><pre><code>FROM python:3.13-slim AS runtime\n</code></pre> What: Start fresh with clean Python image for runtime Why: Multi-stage build benefits: - Builder stage has uv, build tools, cache \u2192 ~500MB - Runtime stage only has Python and your .venv \u2192 ~200MB - 50%+ size reduction by discarding build tools</p>"},{"location":"references/dockerfile-strategy/#non-root-user","title":"Non-Root User","text":"<p><pre><code>RUN groupadd -r app &amp;&amp; useradd -r -g app app\n</code></pre> What: Create a non-root system user named <code>app</code> Why: Security best practice - Containers shouldn't run as root - Limits damage if container is compromised - Standard container orchestration pattern</p> <p>Command breakdown: - <code>groupadd -r app</code>: Create system group (<code>-r</code> assigns GID &lt; 1000 automatically) - <code>useradd -r -g app app</code>: Create system user (<code>-r</code> assigns UID &lt; 1000, <code>-g app</code> assigns to group) - No home directory created, shell defaults to <code>/usr/sbin/nologin</code> (prevents login)</p> <p>Alternative approach: Some Dockerfiles use explicit UIDs (e.g., <code>useradd -u 1001 -g appgroup -m -d /app</code>) for traditional volume sharing (NFS, Docker volumes). We use system users (<code>-r</code>) because: - GCS fuse authentication uses service account IAM, not file UIDs - Simpler - fewer flags, system assigns non-conflicting IDs - No home directory needed - app doesn't write to <code>~/</code></p> <p>For traditional shared volumes with POSIX permissions, explicit UIDs are needed. For object storage (GCS), system users are sufficient.</p> <p>Reference: Depot.dev Python UV Dockerfile shows explicit UID approach</p>"},{"location":"references/dockerfile-strategy/#working-directory-runtime-stage","title":"Working Directory (Runtime Stage)","text":"<p><pre><code>WORKDIR /app\n</code></pre> What: Set working directory again (new stage = new filesystem) Why: Container starts in <code>/app</code> when it runs</p>"},{"location":"references/dockerfile-strategy/#copy-application-from-builder","title":"Copy Application from Builder","text":"<p><pre><code>COPY --from=builder --chown=app:app /app .\n</code></pre> What: Copy entire application directory from builder stage Why: - <code>--from=builder</code>: Get everything from builder's <code>/app</code> - <code>--chown=app:app</code>: Make the <code>app</code> user own it - Destination <code>.</code> uses WORKDIR context (copies to <code>/app</code>) - Includes <code>.venv/</code> (all dependencies + installed package) - Includes <code>src/</code> (application code) - Includes metadata (pyproject.toml, uv.lock, README.md) - Simple, conventional pattern - one COPY instead of multiple - Metadata files are tiny (~30 KB) and harmless - uv is NOT copied (only in builder stage, not needed at runtime)</p>"},{"location":"references/dockerfile-strategy/#runtime-environment","title":"Runtime Environment","text":"<p><pre><code>ENV VIRTUAL_ENV=/app/.venv \\\n    PATH=\"/app/.venv/bin:$PATH\" \\\n    PYTHONUNBUFFERED=1 \\\n    AGENT_DIR=/app/src \\\n    HOST=0.0.0.0 \\\n    PORT=8000\n</code></pre> What: Configure runtime environment Why:</p> Variable Purpose <code>VIRTUAL_ENV=/app/.venv</code> Tell Python which venv to use <code>PATH=\"/app/.venv/bin:$PATH\"</code> Make venv binaries available (python, uvicorn) <code>PYTHONUNBUFFERED=1</code> Don't buffer stdout/stderr (better logs in Docker) <code>HOST=0.0.0.0</code> Explicitly bind all interfaces for containers (server.py defaults to 127.0.0.1) <code>PORT=8000</code> Explicitly set default port (matches EXPOSE and server.py default) <code>AGENT_DIR=/app/src</code> Override agent directory path (see AGENT_DIR section below)"},{"location":"references/dockerfile-strategy/#agent_dir-configuration","title":"AGENT_DIR Configuration","text":"<p>The Problem:</p> <p>When the package is installed in non-editable mode (Docker), the source code is copied to the virtual environment's site-packages: - Local (editable): <code>Path(__file__)</code> \u2192 <code>/path/to/project/src/your_agent_name/server.py</code> - Docker (non-editable): <code>Path(__file__)</code> \u2192 <code>/app/.venv/lib/python3.13/site-packages/your_agent_name/server.py</code></p> <p>Using <code>Path(__file__).parent.parent</code> for <code>AGENT_DIR</code>: - Local: Resolves to <code>/path/to/project/src/</code> \u2705 Correct (contains only your_agent_name/) - Docker: Resolves to <code>/app/.venv/lib/python3.13/site-packages/</code> \u274c Wrong (contains all packages)</p> <p>This causes the ADK web UI to show all installed packages (.dist-info directories) instead of just our agent.</p> <p>The Solution:</p> <pre><code># In server.py - configurable with smart default\nAGENT_DIR = os.getenv(\"AGENT_DIR\", str(Path(__file__).parent.parent))\n</code></pre> <pre><code># In Dockerfile - override for Docker environment\nENV AGENT_DIR=/app/src\n</code></pre> <p>Why this works: - Local dev: No <code>AGENT_DIR</code> env var \u2192 uses <code>Path(__file__).parent.parent</code> \u2192 <code>/path/to/project/src/</code> \u2705 - Docker: <code>AGENT_DIR=/app/src</code> env var set \u2192 overrides default \u2192 <code>/app/src/</code> \u2705 - Both point to directory containing only the agent source code - Configurable via environment variable for other deployment scenarios</p>"},{"location":"references/dockerfile-strategy/#switch-to-non-root","title":"Switch to Non-Root","text":"<p><pre><code>USER app\n</code></pre> What: All subsequent commands run as <code>app</code> user Why: - Container starts as <code>app</code> user at runtime - Can't escalate to root - Security best practice</p>"},{"location":"references/dockerfile-strategy/#expose-port","title":"Expose Port","text":"<p><pre><code>EXPOSE 8000\n</code></pre> What: Document that container listens on port 8000 Why: - Documentation only (doesn't actually publish port) - Tools like docker-compose read this for defaults - Good practice for clarity</p>"},{"location":"references/dockerfile-strategy/#startup-command","title":"Startup Command","text":"<p><pre><code># Run the FastAPI server via main() for unified startup logic (logging, etc.)\nCMD [\"python\", \"-m\", \"your_agent_name.server\"]\n</code></pre> What: Default command when container starts Why: - Calls <code>server.main()</code> for unified startup logic   - Sets up OpenTelemetry observability (traces and logs to Google Cloud)   - Consistent entry point for both local dev (<code>uv run server</code>) and Docker - <code>main()</code> calls <code>uvicorn.run(app, host=os.getenv(\"HOST\", \"127.0.0.1\"), port=...)</code>   - Secure default: 127.0.0.1 (only local connections)   - Dockerfile sets <code>HOST=0.0.0.0</code> to explicitly bind all interfaces for containers   - Respects HOST and PORT environment variables for flexibility - JSON array format (exec form, not shell form) \u2192 more efficient - Can be overridden at runtime</p>"},{"location":"references/dockerfile-strategy/#why-not-use-uv-image-directly","title":"Why Not Use UV Image Directly?","text":"<p>Let's compare the approaches:</p>"},{"location":"references/dockerfile-strategy/#using-uv-image-directly-wont-work","title":"\u274c Using UV Image Directly (Won't Work)","text":"<pre><code>FROM ghcr.io/astral-sh/uv:latest\n\n# ERROR: No shell to run these commands!\nRUN uv sync  # FAILS - distroless has no /bin/sh\nCOPY src ./src  # Works, but then what?\n</code></pre> <p>Problems: - Distroless = no shell \u2192 can't run <code>RUN</code> commands - No package manager \u2192 can't install system dependencies if needed - Image is ~100MB but you can't build anything with it - Designed for copying FROM, not building FROM</p>"},{"location":"references/dockerfile-strategy/#our-approach-multi-stage","title":"\u2705 Our Approach (Multi-Stage)","text":"<pre><code># Builder: Use python:3.13-slim + uv binary\nFROM python:3.13-slim AS builder\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/\n# ... build with full shell/utilities ...\n\n# Runtime: Clean python:3.13-slim + only .venv\nFROM python:3.13-slim AS runtime\nCOPY --from=builder /app/.venv /app/.venv\n</code></pre> <p>Benefits: - Builder has shell + Python + uv \u2192 can build anything - Runtime is minimal \u2192 small image size - Gets official uv binary \u2192 always latest - Best of both worlds</p>"},{"location":"references/dockerfile-strategy/#why-we-copy-instead-of-using-bind-mounts","title":"Why We COPY Instead of Using Bind Mounts","text":""},{"location":"references/dockerfile-strategy/#the-question-should-we-optimize-further","title":"The Question: Should We Optimize Further?","text":"<p>You might wonder: \"Why not use bind mounts to avoid copying dependency files into layers?\"</p> <p>UV's documentation shows bind mount examples, but we deliberately chose the simpler COPY approach. Here's why:</p>"},{"location":"references/dockerfile-strategy/#copy-approach-what-we-use","title":"COPY Approach (What We Use)","text":"<pre><code>COPY pyproject.toml uv.lock ./\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-install-project\n</code></pre> <p>What happens: - Both files copied into builder layer - Docker tracks both files for cache invalidation - When either file changes, layer rebuilds - Cache mount makes rebuilds fast (~2-5s)</p>"},{"location":"references/dockerfile-strategy/#bind-mount-alternative-what-we-dont-use","title":"Bind Mount Alternative (What We Don't Use)","text":"<pre><code>RUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    uv sync --locked --no-install-project\n</code></pre> <p>Potential issues: - Docker doesn't track bind-mounted files - Layer might stay cached even when dependencies change - Requires careful hybrid strategies (COPY some, bind others) - More complex to understand and debug</p>"},{"location":"references/dockerfile-strategy/#our-philosophy-explicit-over-implicit","title":"Our Philosophy: Explicit Over Implicit","text":"<p>Dependency changes SHOULD trigger rebuilds: - \u2705 Clear signal that something material changed - \u2705 Validates that dependencies actually update - \u2705 Predictable Docker caching behavior - \u2705 Easy to understand and debug</p> <p>The \"penalty\" for this explicitness is negligible: - Version bump in pyproject.toml \u2192 rebuilds layer \u2192 cache mount makes it ~2-5s - The cache mount provides 90% of the performance benefit - Simplicity is worth more than saving 3 seconds</p>"},{"location":"references/dockerfile-strategy/#benefits-of-copy-approach","title":"Benefits of COPY Approach","text":"<ul> <li>\u2705 Simple - Standard Docker pattern, easy to understand</li> <li>\u2705 Explicit - Cache invalidation happens when it should</li> <li>\u2705 Reliable - Dependencies always update correctly</li> <li>\u2705 Fast - Cache mount handles performance optimization</li> <li>\u2705 Maintainable - Less explanation needed, easier debugging</li> </ul>"},{"location":"references/dockerfile-strategy/#performance-comparison","title":"Performance Comparison","text":"Approach Builder Size Runtime Size Rebuild Time (code change) Simplicity Notes Single-stage 500MB 500MB 2-5 minutes \u2705 Simple No separation, slow rebuilds Multi-stage + COPY (ours) ~500MB ~200MB 5-10 seconds \u2705 Simple Best balance of performance and clarity Multi-stage + bind mounts ~480MB ~200MB 5-10 seconds \u274c Complex Marginal savings, complex caching UV distroless base Won't work N/A N/A N/A No shell for build commands <p>Key insight: The multi-stage build and cache mount provide 95% of the optimization. Bind mounts add complexity for minimal additional benefit (~20MB, ~2-3s).</p>"},{"location":"references/dockerfile-strategy/#summary","title":"Summary","text":"<p>Why our multi-stage COPY approach?</p> <ol> <li>We can't use distroless uv image as base \u2192 no shell to run build commands</li> <li>We still want official uv binary \u2192 copy it from distroless image into python:slim</li> <li>We separate build from runtime \u2192 smaller final image, faster rebuilds</li> <li>Layer caching optimization \u2192 dependencies cached separately from code</li> <li>Simple COPY over bind mounts \u2192 explicit, reliable, maintainable</li> </ol> <p>Key architectural decisions:</p> Component Approach Why <code>uv</code> binary Copy from distroless Get official binary without distroless constraints Base image <code>python:3.13-slim</code> Need shell + Python for build, minimal for runtime Build pattern Multi-stage 50% size reduction (discard build tools) Dependency caching Cache mount Persist packages across builds (~80% speedup) Dependency files COPY both Explicit cache invalidation, predictable, simple Code separation <code>--no-install-project</code> Dependencies (slow) separate from code (fast) Source copy Copy src/ only Documentation changes don't trigger code layer rebuild README file touch in RUN Optimization: README updates don't invalidate install layer <p>This gives us: - \u2705 Official uv binary (always latest) - \u2705 Full build capabilities (shell, Python, package manager) - \u2705 Minimal runtime image (~200MB vs ~500MB) - \u2705 Fast rebuilds (5-10s for code changes, ~2-5s for dependency changes with cache) - \u2705 Documentation updates don't trigger code/install layer rebuilds - \u2705 Reliable dependency updates (explicit cache invalidation) - \u2705 Simple and maintainable (standard Docker patterns) - \u2705 Maximum security (non-root, minimal attack surface)</p>"},{"location":"references/dockerfile-strategy/#local-development","title":"Local Development","text":"<p>For local development workflow using Docker Compose (recommended), see Docker Compose Workflow Guide.</p> <p>For direct Docker builds without Compose: <pre><code>DOCKER_BUILDKIT=1 docker build -t your-agent-name:latest .\n</code></pre></p>"},{"location":"references/dockerfile-strategy/#references","title":"References","text":""},{"location":"references/dockerfile-strategy/#uv-documentation","title":"UV Documentation","text":"<ul> <li>UV Docker Integration Guide</li> <li>UV Docker Intermediate Layers (Bind Mounts)</li> <li>UV Documentation</li> </ul>"},{"location":"references/dockerfile-strategy/#docker-documentation","title":"Docker Documentation","text":"<ul> <li>Docker Best Practices</li> <li>Docker Multi-Stage Builds</li> <li>Docker RUN Command Reference</li> <li>BuildKit Cache Mounts</li> <li>Docker Bind Mounts</li> </ul>"},{"location":"references/dockerfile-strategy/#docker-compose","title":"Docker Compose","text":"<ul> <li>Docker Compose Workflow Guide - Local development workflow</li> <li>Docker Compose Watch Mode</li> </ul> <p>\u2190 Back to References | Documentation</p>"},{"location":"references/mkdocs-setup/","title":"MkDocs Documentation Site Setup","text":"<p>This template includes a MkDocs documentation site that automatically deploys to GitHub Pages.</p>"},{"location":"references/mkdocs-setup/#enable-github-pages","title":"Enable GitHub Pages","text":"<p>After your first push to <code>main</code>:</p> <ol> <li> <p>Wait for workflow to complete: <pre><code>gh run list --workflow=deploy-docs.yml --limit 1\n</code></pre></p> </li> <li> <p>Configure Pages:</p> </li> <li>Go to Settings \u2192 Pages</li> <li>Source: \"Deploy from a branch\"</li> <li>Branch: <code>gh-pages</code>, Folder: <code>/ (root)</code></li> <li> <p>Save</p> </li> <li> <p>Access your site:</p> </li> <li><code>https://&lt;username&gt;.github.io/&lt;repo-name&gt;/</code></li> </ol> <p>Note</p> <p>GitHub Pages is free for public repos. Private repos require GitHub Pro, Team, or Enterprise.</p>"},{"location":"references/mkdocs-setup/#customize-your-site","title":"Customize Your Site","text":""},{"location":"references/mkdocs-setup/#update-branding","title":"Update Branding","text":"<p>Site name (<code>mkdocs.yml</code>): <pre><code>site_name: Your Project Name Docs\n</code></pre></p> <p>Logo icon (<code>mkdocs.yml</code>): <pre><code>theme:\n  icon:\n    logo: material/robot  # Browse: https://pictogrammers.com/library/mdi/\n</code></pre></p> <p>Colors (<code>docs/stylesheets/extra.css</code>): <pre><code>:root {\n  --md-primary-fg-color:        #5517c0;  /* Main */\n  --md-primary-fg-color--light: #8e5ee8;  /* Hover */\n  --md-primary-fg-color--dark:  #2d0a70;  /* Active */\n  --md-accent-fg-color:         #5517c0;  /* Accent */\n}\n</code></pre></p>"},{"location":"references/mkdocs-setup/#add-your-documentation","title":"Add Your Documentation","text":"<ol> <li>Create markdown files in <code>docs/</code></li> <li> <p>Add to navigation in <code>mkdocs.yml</code>: <pre><code>nav:\n  - Your Section:\n    - Your Doc: your-doc.md\n</code></pre></p> </li> <li> <p>Preview locally: <pre><code>uv sync --group docs\nuv run mkdocs serve  # http://127.0.0.1:8000\n</code></pre></p> </li> </ol>"},{"location":"references/mkdocs-setup/#file-structure","title":"File Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 README.md              # Home page\n\u251c\u2500\u2500 *.md                   # Guide docs\n\u251c\u2500\u2500 stylesheets/\n\u2502   \u2514\u2500\u2500 extra.css         # Custom theme\n\u2514\u2500\u2500 references/           # Reference docs\n</code></pre> <p>GitHub-style alerts work automatically:</p> <p>Note</p> <p>Informational callout example</p>"},{"location":"references/mkdocs-setup/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs - Theme and features</li> <li>Material Icons - Logo icons</li> </ul>"},{"location":"references/protection-strategies/","title":"Protection Strategies Reference","text":"<p>Branch, tag, and environment protection setup and rationale.</p>"},{"location":"references/protection-strategies/#overview","title":"Overview","text":"<p>Three protection layers ensure code quality and security:</p> <ol> <li>Branch Protection (main) - Manual setup, requires PR approval and status checks</li> <li>Tag Protection (v*) - Automated by bootstrap (production mode), prevents tag manipulation</li> <li>Environment Protection (prod-apply) - Manual reviewers, approval gate for production</li> </ol>"},{"location":"references/protection-strategies/#branch-protection-manual-setup-required","title":"Branch Protection (Manual Setup Required)","text":"<p>Main branch protection ensures code quality and prevents accidental commits to main.</p>"},{"location":"references/protection-strategies/#what-to-configure","title":"What to Configure","text":"<ol> <li>Require pull request before merging:</li> <li>Require approvals: 1 (minimum)</li> <li> <p>Dismiss stale reviews when new commits are pushed: \u2705</p> </li> <li> <p>Require status checks to pass:</p> </li> <li>Require branches to be up to date: \u2705</li> <li> <p>Status checks required:</p> <ul> <li><code>Required Checks / required-status</code> (from <code>.github/workflows/required-checks.yml</code>)</li> </ul> </li> <li> <p>Do not allow bypassing settings:</p> </li> <li> <p>Allow specific actors to bypass: Repository admins only</p> </li> <li> <p>Other settings:</p> </li> <li>\u274c Require linear history (optional, not required)</li> <li>\u274c Allow force pushes (keep disabled)</li> <li>\u274c Allow deletions (keep disabled)</li> </ol>"},{"location":"references/protection-strategies/#how-to-configure-github-ui","title":"How to Configure (GitHub UI)","text":"<ol> <li>Go to repository Settings \u2192 Branches</li> <li>Click Add branch protection rule</li> <li>Enter branch name pattern: <code>main</code></li> <li>Configure protection settings:</li> <li>\u2705 Require a pull request before merging<ul> <li>Required approvals: <code>1</code></li> <li>\u2705 Dismiss stale pull request approvals when new commits are pushed</li> </ul> </li> <li>\u2705 Require status checks to pass before merging<ul> <li>\u2705 Require branches to be up to date before merging</li> <li>Search for and select: <code>Required Checks / required-status</code></li> </ul> </li> <li>\u2705 Do not allow bypassing the above settings<ul> <li>Allow specific actors to bypass: Repository admins (optional)</li> </ul> </li> <li>\u274c Require linear history (optional)</li> <li>\u274c Allow force pushes (disabled)</li> <li>\u274c Allow deletions (disabled)</li> <li>Click Create or Save changes</li> </ol>"},{"location":"references/protection-strategies/#verify","title":"Verify","text":"<p>GitHub UI: 1. Settings \u2192 Branches 2. Confirm <code>main</code> branch protection rule exists 3. Check rule shows: 1 required approval, required status checks, dismiss stale reviews</p> <p>CLI: <pre><code>gh api repos/:owner/:repo/branches/main/protection | jq '{\n  pr_reviews: .required_pull_request_reviews.required_approving_review_count,\n  dismiss_stale: .required_pull_request_reviews.dismiss_stale_reviews,\n  status_checks: .required_status_checks.contexts\n}'\n</code></pre></p>"},{"location":"references/protection-strategies/#why-manual","title":"Why Manual?","text":"<p>Branch protection rules are repository-wide policy decisions that vary by team workflow. Terraform would override manual adjustments on every apply.</p> <p>Different teams may want: - Different approval counts (1 vs 2) - Different required status checks - Different bypass permissions - CODEOWNERS enforcement</p> <p>Bootstrap can't predict these preferences, so we make it a manual configuration step.</p>"},{"location":"references/protection-strategies/#tag-protection-automated","title":"Tag Protection (Automated)","text":"<p>Production tag protection is automatically configured by bootstrap in production mode.</p>"},{"location":"references/protection-strategies/#what-bootstrap-creates","title":"What Bootstrap Creates","text":"<p>Bootstrap (<code>terraform/bootstrap/prod/</code>) creates a <code>github_repository_ruleset</code> that protects <code>v*</code> tags:</p> <p>Protection rules: - Prevents: tag creation, update, deletion by non-admins - Prevents: force pushes to tags - Allows: Repository admins to bypass (for tag management)</p> <p>Ruleset configuration: - Name: \"Production Release Tag Protection\" - Target: Tags matching <code>refs/tags/v*</code> - Enforcement: Active</p>"},{"location":"references/protection-strategies/#verify_1","title":"Verify","text":"<p>GitHub UI: 1. Settings \u2192 Rules \u2192 Rulesets 2. Confirm ruleset exists: Production Release Tag Protection 3. Check details:    - Enforcement: Active    - Target: Tags    - Patterns: <code>refs/tags/v*</code></p> <p>CLI: <pre><code>gh api repos/:owner/:repo/rulesets | jq '.[] | {name, enforcement, target}'\n</code></pre></p> <p>Expected output: <pre><code>{\n  \"name\": \"Production Release Tag Protection\",\n  \"enforcement\": \"active\",\n  \"target\": \"tag\"\n}\n</code></pre></p>"},{"location":"references/protection-strategies/#why-automated","title":"Why Automated?","text":"<p>Tag protection is environment-specific infrastructure: - Production mode requires tag protection (tags trigger prod deployments) - Dev-only mode doesn't need tag protection (tags just version dev deployments)</p> <p>Bootstrap enforces this consistently based on deployment mode. No manual decision needed.</p>"},{"location":"references/protection-strategies/#environment-protection-manual-approval-setup-required","title":"Environment Protection (Manual Approval Setup Required)","text":"<p>The <code>prod-apply</code> GitHub Environment requires manual approval before production deployments.</p>"},{"location":"references/protection-strategies/#what-bootstrap-creates_1","title":"What Bootstrap Creates","text":"<p>Automated by bootstrap (production mode): - GitHub Environment: <code>prod-apply</code> - Environment variables scoped to <code>prod-apply</code></p> <p>Bootstrap does NOT configure: - Required reviewers (manual setup required) - Wait timer - Deployment branch policy</p>"},{"location":"references/protection-strategies/#what-you-must-configure-manually","title":"What You Must Configure Manually","text":"<p>Required reviewers - Users or teams who can approve production deployments.</p> <p>How to configure:</p> <ol> <li>Go to Settings \u2192 Environments \u2192 prod-apply</li> <li>Under Deployment protection rules:</li> <li>\u2705 Check Required reviewers</li> <li>Click Add reviewers search box</li> <li>Add individual users (e.g., tech leads, SREs) or teams (e.g., @org/platform-team)</li> <li>Optional: Check Prevent self-review (requires different approver than workflow trigger)</li> <li>Optional: Check Wait timer and set delay (e.g., 5 minutes before allowing approval)</li> <li>Optional: Check Allow administrators to bypass (admin override for emergencies)</li> <li>Click Save protection rules</li> <li>Under Deployment branches and tags:</li> <li>Click dropdown \u2192 Select Selected branches and tags</li> <li>Add branch rule: <code>main</code> (only main branch can trigger prod deployments)</li> <li>Add tag rule: <code>v*.*.*</code> or <code>v*</code> (version tags can trigger prod deployments)</li> <li>Click Save protection rules</li> </ol>"},{"location":"references/protection-strategies/#verify_2","title":"Verify","text":"<p>GitHub UI: 1. Settings \u2192 Environments 2. Confirm <code>prod-apply</code> environment exists 3. Click on <code>prod-apply</code> to view details 4. Check Deployment protection rules shows:    - Required reviewers configured    - Wait timer (if enabled)    - Self-review prevention (if enabled) 5. Check Deployment branches and tags shows:    - Branch rule: <code>main</code>    - Tag rule: <code>v*.*.*</code> or <code>v*</code></p> <p>CLI: <pre><code>gh api repos/:owner/:repo/environments/prod-apply | jq '{\n  name: .name,\n  protection_rules: .protection_rules\n}'\n</code></pre></p> <p>Expected output shows non-empty <code>reviewers</code> array.</p>"},{"location":"references/protection-strategies/#why-manual_1","title":"Why Manual?","text":"<p>Reviewer selection is a human security decision that changes over time:</p> <ul> <li>Team composition changes (new hires, departures, role changes)</li> <li>Responsibility shifts (different teams own production approvals)</li> <li>Organization structure evolves</li> </ul> <p>Bootstrap creates the environment, but your team decides who can approve production deployments. The bootstrap module explicitly ignores reviewer changes (<code>lifecycle.ignore_changes</code>) to prevent Terraform from overwriting your manual selections.</p>"},{"location":"references/protection-strategies/#how-approvals-work","title":"How Approvals Work","text":""},{"location":"references/protection-strategies/#production-deployment-flow","title":"Production Deployment Flow","text":"<ol> <li>Repository Admin (allowed to bypass tag protection ruleset) pushes tag matching allowed pattern: <code>git tag -a v1.0.0 -m \"Release v1.0.0\" &amp;&amp; git push origin v1.0.0</code></li> <li>GitHub Actions workflow starts (tag must match <code>v*.*.*</code> or <code>v*</code> pattern configured in environment)</li> <li>Jobs run sequentially:</li> <li>Resolve image digest from stage</li> <li>Promote image to prod registry</li> <li>Run Terraform plan for prod</li> <li>prod-apply job waits for approval (requires <code>prod-apply</code> Environment configured reviewer from approved list)</li> <li>Configured reviewer sees notification: \"Review deployments waiting\"</li> <li>Reviewer checks:</li> <li>Deployment logs and outputs</li> <li>Plan changes</li> <li>Staging environment validation</li> <li>Reviewer approves or rejects</li> <li>If approved: Terraform apply runs, deploys to production</li> <li>If rejected: Workflow fails, no deployment</li> </ol> <p>Note: If tag doesn't match allowed pattern or workflow triggered from non-main branch, deployment to <code>prod-apply</code> environment will be blocked by environment rules.</p>"},{"location":"references/protection-strategies/#approving-a-deployment","title":"Approving a Deployment","text":"<p>GitHub UI: 1. Go to Actions tab 2. Click on the waiting workflow run 3. Click Review deployments button 4. Select environment: <code>prod-apply</code> 5. Add comment (optional but recommended): \"LGTM - staging validation passed\" 6. Click Approve and deploy or Reject</p> <p>CLI: <pre><code># View pending deployments\ngh run list --workflow=ci-cd.yml | grep \"waiting\"\n\n# Approve deployment (requires GitHub UI, no CLI support for approval)\n</code></pre></p> <p>Note: GitHub CLI doesn't support approving deployments. Use the GitHub UI.</p>"},{"location":"references/protection-strategies/#protection-strategy-summary","title":"Protection Strategy Summary","text":"Protection Type When Why Branch (main) Manual After repo creation Team-specific workflow preferences Tag (v*) Automated Bootstrap (prod mode) Environment-specific requirement Environment (prod-apply) Manual reviewers After prod bootstrap Human security decision, changes over time <p>\u2190 Back to References | Documentation</p>"},{"location":"references/testing/","title":"Testing Strategy","text":"<p>Detailed testing patterns, organization, and requirements for the project.</p>"},{"location":"references/testing/#coverage-requirements","title":"Coverage Requirements","text":"<p>100% coverage required on production code.</p> <p>Included: - All code in <code>src/</code> except explicitly excluded files (see below)</p> <p>Excluded: - <code>server.py</code> (FastAPI entrypoint - validate server behavior with integration tests) - <code>**/agent.py</code> (pure ADK configuration - tests are the upstream project's responsibility) - <code>**/prompt.py</code> (prompt templates - validate agent behavior with evaluations) - <code>**/__init__.py</code> (module initialization)</p>"},{"location":"references/testing/#test-organization","title":"Test Organization","text":""},{"location":"references/testing/#directory-structure","title":"Directory Structure","text":"<p>Tests mirror source structure:</p> <pre><code>tests/\n  conftest.py                    # Shared fixtures, mocks, and test environment setup\n  test_callbacks.py              # Tests for src/agent_foundation/callbacks.py\n  test_tools.py                  # Tests for src/agent_foundation/tools.py\n  ...\n</code></pre>"},{"location":"references/testing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Files: <code>test_&lt;module&gt;.py</code> mirroring source module name</li> <li>Functions: <code>test_&lt;what&gt;_&lt;condition&gt;_&lt;expected&gt;</code></li> <li>Classes: Group related tests for the same module/class (style preference)</li> </ul>"},{"location":"references/testing/#shared-fixtures","title":"Shared Fixtures","text":"<p>All reusable fixtures go in <code>tests/conftest.py</code>: - Type hint fixture definitions with both parameters and return types - Use pytest-mock type aliases for returns: <code>MockType</code>, <code>AsyncMockType</code> - Factory pattern (not context managers)</p>"},{"location":"references/testing/#fixture-patterns","title":"Fixture Patterns","text":""},{"location":"references/testing/#type-hints","title":"Type Hints","text":"<p>Fixture definitions (strict in conftest.py): <pre><code>from pytest_mock import MockerFixture\nfrom pytest_mock.plugin import MockType\n\n@pytest.fixture\ndef mock_session(mocker: MockerFixture) -&gt; MockType:\n    \"\"\"Create a mock ADK session.\"\"\"\n    return mocker.MagicMock(spec=...)\n</code></pre></p> <p>Test functions (relaxed): - Don't type hint custom fixtures (pytest handles DI by name) - Optional type hints on built-in fixtures for IDE support</p>"},{"location":"references/testing/#environment-mocking","title":"Environment Mocking","text":"<p>Base test environment: Set in <code>pytest_configure()</code> using direct <code>os.environ</code> assignments (see pytest_configure section below).</p> <p>One-off overrides in specific tests: Use <code>mocker.patch.dict</code> when you need to override or add environment variables for a single test:</p> <pre><code>def test_config_with_custom_region(mocker: MockerFixture) -&gt; None:\n    \"\"\"Test configuration with non-default region.\"\"\"\n    # Override just the region for this test\n    mocker.patch.dict(os.environ, {\"GOOGLE_CLOUD_LOCATION\": \"us-west1\"})\n    # Test config loading with custom region\n</code></pre> <p>This approach keeps base env vars in <code>pytest_configure()</code> and only uses mocking for test-specific overrides.</p>"},{"location":"references/testing/#adk-mock-strategy","title":"ADK Mock Strategy","text":""},{"location":"references/testing/#using-conftest-fixtures","title":"Using Conftest Fixtures","text":"<p>Prefer fixtures from <code>conftest.py</code> for standard mocks: - <code>mock_state</code> - ADK state object - <code>mock_session</code> - ADK session - <code>mock_context</code> - ReadonlyContext with user_id</p>"},{"location":"references/testing/#when-to-import-mock-classes","title":"When to Import Mock Classes","text":"<p>Import mock classes directly only when: - Creating fixtures for many variants adds more complexity than value - Guideline: &gt;3 variants \u2192 import class; standard cases \u2192 use/add fixture</p>"},{"location":"references/testing/#mirror-real-interfaces","title":"Mirror Real Interfaces","text":"<p>ADK mocks must exactly mirror real ADK interfaces: - Use <code>spec=</code> parameter to enforce interface - Include all properties and methods used in code - Match return types and signatures</p>"},{"location":"references/testing/#pytest_configure","title":"pytest_configure()","text":"<p>Special case: unittest.mock before pytest-mock available</p> <p><code>pytest_configure()</code> runs before test collection, so pytest-mock isn't available yet. Use <code>unittest.mock</code> here for setup that must happen before tests load:</p> <pre><code>import os\nfrom unittest.mock import MagicMock, patch\n\ndef pytest_configure() -&gt; None:\n    \"\"\"Configure test environment before test collection.\"\"\"\n    # Mock google.auth before modules import it\n    with patch(\"google.auth.default\", return_value=(MagicMock(), \"project\")):\n        pass\n\n    # Set environment directly (not setdefault)\n    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"test-project\"\n    os.environ[\"AGENT_NAME\"] = \"test-agent\"\n</code></pre> <p>See <code>tests/conftest.py</code> for complete example with detailed comments.</p>"},{"location":"references/testing/#pydantic-validation-testing","title":"Pydantic Validation Testing","text":""},{"location":"references/testing/#validation-timing","title":"Validation Timing","text":"<p>Pydantic validates at model creation, not at property access:</p> <pre><code># Validation happens here \u2713\nconfig = ServerEnv.model_validate(env_dict)\n\n# NOT here \u2717\nvalue = config.some_property\n</code></pre>"},{"location":"references/testing/#testing-validation","title":"Testing Validation","text":"<p>Expect <code>ValidationError</code> at model creation:</p> <pre><code>import pytest\nfrom pydantic import ValidationError\n\ndef test_config_invalid_project_id():\n    \"\"\"Test that empty project ID raises validation error.\"\"\"\n    env_dict = {\"GOOGLE_CLOUD_PROJECT\": \"\"}\n\n    with pytest.raises(ValidationError):\n        ServerEnv.model_validate(env_dict)\n</code></pre>"},{"location":"references/testing/#what-to-test","title":"What to Test","text":""},{"location":"references/testing/#behaviors","title":"Behaviors","text":"<p>Test function outputs and state changes: - Return values are correct - State is modified as expected - Side effects occur (logging, callbacks)</p>"},{"location":"references/testing/#error-conditions","title":"Error Conditions","text":"<p>Test invalid inputs and edge cases: - Invalid parameter types - Missing required values - Boundary conditions (empty, max, negative)</p>"},{"location":"references/testing/#integration-points","title":"Integration Points","text":"<p>Test connections between components: - Callbacks are registered and invoked - Tools are added to agent correctly - Configuration flows through system</p>"},{"location":"references/testing/#mypy-override-for-tests","title":"Mypy Override for Tests","text":"<p>Tests use relaxed type checking for pragmatism:</p> <pre><code>[[tool.mypy.overrides]]\nmodule = \"tests.*\"\ndisable_error_code = [\"arg-type\"]\n</code></pre> <p>Still type hint <code>conftest.py</code> fully, but test functions can be more flexible.</p>"},{"location":"references/testing/#running-tests","title":"Running Tests","text":"<pre><code># Full coverage report\nuv run pytest --cov --cov-report=term-missing\n\n# HTML report for detailed view\nuv run pytest --cov --cov-report=html\nopen htmlcov/index.html\n\n# Specific tests\nuv run pytest tests/test_integration.py -v\nuv run pytest tests/test_file.py::test_name -v\n\n# Watch mode (requires pytest-watch)\nuv run ptw\n</code></pre>"},{"location":"references/testing/#examples","title":"Examples","text":"<p>See <code>tests/conftest.py</code> and existing test files for complete patterns: - Fixture factories - ADK mocks - Environment mocking - Pydantic validation tests - Async test patterns (with pytest-asyncio)</p> <p>\u2190 Back to References | Documentation</p>"}]}